<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://timashov.ai/feed.xml" rel="self" type="application/atom+xml"/><link href="https://timashov.ai/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-04-29T10:42:33+00:00</updated><id>https://timashov.ai/feed.xml</id><title type="html">Aleksandr Timashov</title><subtitle>The personal page of Aleksandr Timashov </subtitle><entry><title type="html">How Computers Store Data in Memory: Brief Intro</title><link href="https://timashov.ai/blog/2025/data-in-memory/" rel="alternate" type="text/html" title="How Computers Store Data in Memory: Brief Intro"/><published>2025-04-26T09:00:00+00:00</published><updated>2025-04-26T09:00:00+00:00</updated><id>https://timashov.ai/blog/2025/data%20in%20memory</id><content type="html" xml:base="https://timashov.ai/blog/2025/data-in-memory/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>When <strong>we communicate</strong> with each other, we use complex <strong>natural languages</strong> like English, Portuguese, or Russian. The language we use depends on our location or the community around us. When we see something, we perceive information visually through our eyes and brain. And when we count numbers, most people are familiar with one system — the <strong>decimal system</strong>, where each position can be represented by one of 10 digits: <code class="language-plaintext highlighter-rouge">0, 1, 2, ..., 9</code>. So, at least three independent ways exist to perceive and exchange information.<br/> <strong>Computers</strong>, however, operate very differently — and in some sense, much more simply. They do not have a native natural language or vision system at their core. Instead, they use only the <strong>binary system</strong>, composed of <code class="language-plaintext highlighter-rouge">0</code> and <code class="language-plaintext highlighter-rouge">1</code>. Each “box” of information can contain either a 0 or a 1, and this unit is called a <strong>bit</strong>. Historically, 8 bits make up a <strong>byte</strong>.</p> <p><img src="/assets/img/bits_bytes.png" alt="Img.1: Explanation of Bits and Bytes" style="width:100%;"/></p> <p>Higher-level constructs, such as <strong>Natural Language Models</strong> (<em>LLaMA, ChatGPT, Grok, etc.</em>) and <strong>Vision Systems</strong> (<em>YOLO, Faster R-CNN, Stable Diffusion, etc.</em>), are built on top of this fundamental binary representation.</p> <p>Understanding how computers “speak” at the basic level is critical for building intuition in Deep Learning. In this post, I learn how computers represent and store information, how it connects to data types, and why these foundations matter when designing and optimizing AI models.</p> <hr/> <h2 id="decimal-vs-binary">Decimal vs Binary</h2> <p>The <strong>decimal system</strong> is the numeral system we use in daily life. Each digit can take one of <em>10</em> different values: <code class="language-plaintext highlighter-rouge">0, 1, 2, ..., 9</code>, ordered naturally from smallest to largest. To construct numbers, we use powers of <em>10</em>: when increasing past <em>9</em>, we reset the digit to <em>0</em> and add <em>1</em> to the next higher place value. The <strong>binary system</strong> works similarly — but instead of <em>10</em> possible values, each digit (bit) can only be <code class="language-plaintext highlighter-rouge">0</code> or <code class="language-plaintext highlighter-rouge">1</code>. Here, the base is <em>2</em>, and each digit represents a power of <em>2</em>.</p> <p><img src="/assets/img/nums_seq.png" alt="Img.2: Sequence of numbers in Decimal and Binary systems" style="width:100%;"/></p> <p>Fractional numbers follow the same idea: each digit after the floating point represents a negative power of the base - <em>10</em> for decimal numbers, and <em>2</em> for binary numbers. The algorithm for <strong>converting a fractional number from decimal to binary</strong> is iterative and slightly different for the integer and fractional parts:</p> <ul> <li>For the <strong>integer part</strong>, divide the number by <em>2</em>, record the remainder (<em>0</em> or <em>1</em>), and continue dividing the quotient by <em>2</em> <strong>until</strong> it becomes <em>0</em>.</li> <li>For the <strong>fractional part</strong>, multiply the fraction by <em>2</em>, record the integer part (<em>0</em> or <em>1</em>), and repeat the process with the remaining fractional part. The process <strong>stops when</strong> the fraction becomes 0 or when the desired precision is reached.</li> </ul> <p>Below is the step-by-step calculation for the number <code class="language-plaintext highlighter-rouge">13.875</code>:</p> <p><img src="/assets/img/binary_repr.png" alt="Img.3: Representation of fractional numbers in Decimal and Binary systems" style="width:100%;"/></p> <p>We’re used to the decimal system; however, the binary system follows the exact same logic — just with <em>2</em> symbols instead of <em>10</em>. Inside a computer, tiny transistors act as switches that can be either ON or OFF, determined by voltage levels. It’s much easier and more reliable to distinguish just two states — high vs. low voltage — than to detect multiple. This simplicity and robustness is <strong>why all information in computers is stored in binary</strong>.</p> <h2 id="main-data-types">Main Data Types</h2> <p>At the core of computing, we work with a few basic types:</p> <ul> <li><strong>Integer (<code class="language-plaintext highlighter-rouge">int</code>)</strong>: Whole numbers, like <code class="language-plaintext highlighter-rouge">-1</code>, <code class="language-plaintext highlighter-rouge">0</code>, <code class="language-plaintext highlighter-rouge">42</code>.</li> <li><strong>Floating-point (<code class="language-plaintext highlighter-rouge">float</code>)</strong>: Numbers with decimals, like <code class="language-plaintext highlighter-rouge">3.14</code>, <code class="language-plaintext highlighter-rouge">-0.001</code>.</li> <li><strong>Boolean (<code class="language-plaintext highlighter-rouge">bool</code>)</strong>: <code class="language-plaintext highlighter-rouge">True</code> or <code class="language-plaintext highlighter-rouge">False</code>, used in logical decisions.</li> <li><strong>Character (<code class="language-plaintext highlighter-rouge">char</code>)</strong>: Single characters like <code class="language-plaintext highlighter-rouge">'a'</code>, <code class="language-plaintext highlighter-rouge">'Z'</code>, <code class="language-plaintext highlighter-rouge">'#'</code>.</li> <li><strong>String (<code class="language-plaintext highlighter-rouge">str</code>)</strong>: Sequences of characters like <code class="language-plaintext highlighter-rouge">"Hello"</code>.</li> </ul> <p>Each type has a size and memory footprint. For example:</p> <ul> <li><code class="language-plaintext highlighter-rouge">int32</code> (32-bit integer) or <code class="language-plaintext highlighter-rouge">float64</code> (64-bit floating-point).</li> </ul> <hr/> <h2 id="data-types-used-in-deep-learning">Data Types Used in Deep Learning</h2> <p>Deep learning relies heavily on numerical tensors — multi-dimensional arrays where <strong>type precision</strong> matters for memory and speed.</p> <p>Common types:</p> <ul> <li><strong><code class="language-plaintext highlighter-rouge">float32</code></strong>: 32-bit floating point — standard for most models.</li> <li><strong><code class="language-plaintext highlighter-rouge">float16</code></strong>: Half-precision — used for faster computation (e.g., NVIDIA Tensor Cores).</li> <li><strong><code class="language-plaintext highlighter-rouge">bfloat16</code></strong>: Brain Floating Point, optimized for AI workloads.</li> <li><strong><code class="language-plaintext highlighter-rouge">int8</code></strong>: 8-bit integers — used in quantized models to reduce model size.</li> </ul> <p>Choosing the right type can influence model <strong>speed</strong>, <strong>accuracy</strong>, and <strong>training stability</strong>.</p> <hr/> <h2 id="quick-back-of-envelope-calculation">Quick Back-of-Envelope Calculation</h2> <p>Suppose you have a tensor with shape <code class="language-plaintext highlighter-rouge">(batch_size=64, channels=3, height=224, width=224)</code>, typical for an image classification model.</p> <p>How much memory would it consume with different types?</p> <ul> <li> <p>Number of elements:<br/> <code class="language-plaintext highlighter-rouge">64 × 3 × 224 × 224 = 9,633,792</code></p> </li> <li> <p>Memory usage:</p> <ul> <li><code class="language-plaintext highlighter-rouge">float32</code> (4 bytes per value):<br/> <code class="language-plaintext highlighter-rouge">9,633,792 × 4 = ~38.5 MB</code></li> <li><code class="language-plaintext highlighter-rouge">float16</code> (2 bytes per value):<br/> <code class="language-plaintext highlighter-rouge">9,633,792 × 2 = ~19.2 MB</code></li> <li><code class="language-plaintext highlighter-rouge">int8</code> (1 byte per value):<br/> <code class="language-plaintext highlighter-rouge">9,633,792 × 1 = ~9.6 MB</code></li> </ul> </li> </ul> <p><strong>Notice</strong> how changing the data type immediately halves or quarters the memory footprint.</p> <hr/> <h2 id="conclusion">Conclusion</h2> <p>Understanding data types is the foundation of building efficient AI systems.<br/> It affects not only how we design models but also how fast and how large they can be.</p> <p>In future posts, we’ll dive deeper into <strong>precision trade-offs</strong> and <strong>mixed-precision training</strong> strategies.</p> <h2 id="image-slider">Image Slider</h2> <p>This is a simple image slider. It uses the <a href="https://swiperjs.com/">Swiper</a> library. Check the <a href="https://swiperjs.com/demos">examples page</a> for more information of what you can achieve with it.</p> <swiper-container keyboard="true" navigation="true" pagination="true" pagination-clickable="true" pagination-dynamic-bullets="true" rewind="true"> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/9-480.webp 480w,/assets/img/9-800.webp 800w,/assets/img/9-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/9.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/7-480.webp 480w,/assets/img/7-800.webp 800w,/assets/img/7-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/7.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/8-480.webp 480w,/assets/img/8-800.webp 800w,/assets/img/8-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/8.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/10-480.webp 480w,/assets/img/10-800.webp 800w,/assets/img/10-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/10.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/12-480.webp 480w,/assets/img/12-800.webp 800w,/assets/img/12-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/12.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> </swiper-container> <h2 id="image-comparison-slider">Image Comparison Slider</h2> <p>This is a simple image comparison slider. It uses the <a href="https://img-comparison-slider.sneas.io/">img-comparison-slider</a> library. Check the <a href="https://img-comparison-slider.sneas.io/examples.html">examples page</a> for more information of what you can achieve with it.</p> <img-comparison-slider> <figure slot="first"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/prof_pic-480.webp 480w,/assets/img/prof_pic-800.webp 800w,/assets/img/prof_pic-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/prof_pic.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure slot="second"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/prof_pic_color-480.webp 480w,/assets/img/prof_pic_color-800.webp 800w,/assets/img/prof_pic_color-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/prof_pic_color.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </img-comparison-slider>]]></content><author><name></name></author><category term="cs336"/><category term="pre-dl"/><summary type="html"><![CDATA[fp32, fp16, and more]]></summary></entry><entry><title type="html">PyG Implementation of EDP-GNN: Generation via Score-Based Generative Modeling</title><link href="https://timashov.ai/blog/2024/pyg-implementation-of-edp-gnn-generation-via-score-based-generative-modeling/" rel="alternate" type="text/html" title="PyG Implementation of EDP-GNN: Generation via Score-Based Generative Modeling"/><published>2024-10-24T21:02:32+00:00</published><updated>2024-10-24T21:02:32+00:00</updated><id>https://timashov.ai/blog/2024/pyg-implementation-of-edp-gnn-generation-via-score-based-generative-modeling</id><content type="html" xml:base="https://timashov.ai/blog/2024/pyg-implementation-of-edp-gnn-generation-via-score-based-generative-modeling/"><![CDATA[<p>By Yiwen Chen, Aleksandr Timashov, and Yue (Andy) Zhang as part of the Stanford CS224W course project.</p> <h3>Table of Contents</h3> <ol><li>Motivation</li><li>Graph Neural Networks</li><li>Score-Based Generative Modeling</li><li>Annealed Langevin dynamic sampling</li><li>Noise Scheduler Utility</li><li>Edgewise Dense Prediction Graph Neural Network (EDP-GNN)</li><li>Conclusion</li><li>References</li></ol> <h3>Motivation</h3> <p>Recent advancements in generative models, particularly score-based generative modeling, have made significant strides across various domains. Yet, a key challenge in graph generation models remains: the lack of permutation invariance. This blog post introduces the EDP-GNN architecture, a permutation invariant approach to graph generation. EDP-GNN, once trained on a graph dataset, enables the generation of new graphs that mirror the input distribution using annealed Langevin dynamics sampling.<br/>In the context of score-based generative modeling, we focus on learning the gradient of log data density (score function). This approach permits more expressive architectures as it bypasses the need for score normalization. By leveraging inherently permutation-invariant structures like graph neural networks, we ensure consistent outputs for equivalent adjacency matrices. The EDP-GNN architecture exemplifies this, learning graph distribution scores while maintaining permutation invariance. Our discussion will also cover the practical aspects of training and sampling in score-based graph generation.</p> <h3>Graph Neural Networks</h3> <h4>Use cases</h4> <p>Graph Neural Networks (GNNs) are a major innovation in machine learning, tailored to analyze data structured like networks or graphs. This makes them ideal for a wide range of applications. This includes things like social networks and the way proteins fold, which are naturally structured as interconnected points. GNNs are crucial in areas like traffic management, where they help optimize routes by understanding complex road networks. They also play a significant role in finance, for fraud detection by analyzing transaction networks, and in recommendation systems, where they suggest products based on a user’s network of interests. What makes GNNs stand out is their ability to interpret the intricate web of relationships in these data sets, a task that conventional neural networks find challenging. By harnessing the power of connections and patterns within the data, GNNs offer insightful and effective solutions across these diverse fields, demonstrating their versatility and importance in handling graph-based information.</p> <h4>Permutation invariance</h4> <p>One of the key distinctions of GNNs from other neural network architectures is their ability to maintain permutation invariance. This means that the output of a GNN does not change even if the order of the nodes in the input graph is altered. This property is crucial in graph data since the ordering of nodes in a graph is often arbitrary and does not convey meaningful information. In a graph with N nodes, there can be up to N! (N factorial) different adjacency matrices representing the same graph. The invariance to node permutation allows GNNs to generalize better and be applied to various graph structures without losing effectiveness. The versatility and adaptability of GNNs in handling diverse and complex graph-structured data underscore their growing importance in the field of artificial intelligence.</p> <h3>Score-Based Generative Modeling</h3> <p>Score-based models for graph analysis use neural networks to model a unique aspect called the score function. This function essentially represents the gradient of the logarithm of a graph’s data distribution. The advantage of using the score function lies in its simplicity: unlike direct density modeling, which requires complex normalization to ensure probabilities sum to one, score functions bypass this requirement.</p> <p>These models are structured around a sequence of noise levels, denoted as <em>σ</em>₁, <em>σ</em>₂​, …, <em>σ</em>ₗ, with each level being progressively smaller (<em>σ</em>₁ &lt; <em>σ</em>₂​ &lt; … &lt; <em>σ</em>ₗ). The model is conditioned on these noise levels, using them to gradually shape the output. Specifically, the model generates adjacency matrices by starting from noise and applying a technique known as annealed Langevin dynamics. This process, which we’ll explore in more detail, is key to understanding how score-based models efficiently and effectively handle graph data.</p> <h3>Langevin dynamic sampling</h3> <p>Once an EDP-GNN model is trained on a dataset of graphs, we can use annealed Langevin dynamics to sample new graphs from the model.<br/>For a given EDP-GNN score model <strong><em>s(x, σ)</em></strong>, the Langevin update used for sampling is:</p> <ol><li>Sample noise <strong><em>zₜ</em></strong> from Gaussian <strong><em>N(0, I)</em></strong></li><li>Update <strong><em>xₜ₊₁ = xₜ + α/2 s(x, σ) + √(α) zₜ</em></strong></li></ol> <p>We see that there are two key components in the update procedure. The term <strong><em>α/2 s(x, σ)</em></strong> is the gradient update, moving <strong><em>xₜ</em></strong> towards more likely regions of the learned data distribution. The term <strong><em>√(α) zₜ</em></strong> adds some noise to the sampling — without it, samples will converge to the local optima and you would not get correct samples. This procedure is run for a large number of iterations, <strong>T</strong>, while the step size alpha should be sufficiently small.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/864/1*tJuk1mTpawoBuVcPptDOAA.gif"/><figcaption>Annealed Langevin dynamics © Yang Song</figcaption></figure> <p>To extend the Langevin sampling procedure to annealed Langevin sampling, we make use of the noise conditioning of our EDP-GNN score model. We start by using the plain Langevin procedure to sample from a very noisy distribution <strong><em>s(x, σ_L)</em></strong>. The samples produced are then used to initialize the sampling from <strong><em>s(x, σ_{L-1})</em></strong>, which provides slightly less noisy gradient updates. We gradually decrease the noise and refine the samples until <strong><em>s(x, σ</em></strong>₁<strong><em>)</em></strong>, which has the most accurate representation of the input data distribution.<br/>The full pseudocode for annealed Langevin dynamics sampling as follows:</p> <pre>Initialize x_1 from a simple prior distribution, eg. Gaussian<br />For sigma_i in sigma_L, …, sigma_1:<br /> Define the step size alpha_i  = epsilon * sigma_i^2 / sigma_L^2<br /> For t in 1, … T:<br />  Sample z_t from N(0, I)<br />  Update x_{t+1} = x_t + alpha/2 * s(x, sigma) + sqrt(alpha) z_t<br /> Initialize for the next noise level x_1 = x_t<br />Return x_T</pre> <p>In pytorch_geometric, annealed Langevin dynamics sampling for EDP-GNN (and other score-based graph generative models) is implemented in <em>torch_geometric.utils.langevin.</em> You can use it like this:</p> <pre>import torch_geometric.utils.langevin as langevin<br />score_model = … # EDP-GNN model<br />node_features = … # num_nodes x feature_dim matrix of node features<br />adjs, node_flags = langevin.generate_initial_sample(<br />  batch_size=5, num_nodes=10<br />)<br /><br />def score_func(adjs, node_flags):<br /> return score_model(node_features, adjs, node_flags)<br /><br />new_adjs = langevin.sample(<br />  score_func, adjs, node_flags, num_steps=5000, quantize=True<br />)</pre> <p>In the pytorch_geometric implementation, a couple of graph-specific modifications are made to the annealed Langevin dynamics procedure:</p> <ol><li>The adjacency matrices produced by this procedure have continuous values, but in practice, we often wish to sample graphs with discrete edge values. We can do this at the end, by simply defining a threshold to binarize or quantize the adjacency matrix values.</li><li>Since adjacency matrices are symmetric, the initial adjacency matrix x_1 and noise term z_t also must be made symmetric.</li></ol> <h3>Noise Scheduler Utility</h3> <p>The noise scheduler is a commonly used utility for generating noise levels in score models and other diffusion-based models. In score-based generative modeling framework, noise is systematically added to the data at various levels, denoted as sigma values, during the training process. This approach is essential for learning the noise-conditional score model. We have implemented the noise scheduler as described in “Generative Modeling by Estimating Gradients of the Data Distribution”, which specifically generates noise levels on a logarithmic scale.</p> <pre>def get_smld_sigma_schedule(<br />    sigma_min: float,<br />    sigma_max: float,<br />    num_scales: int,<br />    dtype: Optional[torch.dtype] = None,<br />    device: Optional[torch.device] = None,<br />) -&gt; Tensor:<br />    r&quot;&quot;&quot;Generates a set of noise values on a logarithmic scale for &quot;Score<br />    Matching with Langevin Dynamics&quot; from the `&quot;Generative Modeling by<br />    Estimating Gradients of the Data Distribution&quot;<br />    &lt;https://arxiv.org/abs/1907.05600&gt;`_ paper.<br /><br />    This function returns a vector of sigma values that define the schedule of<br />    noise levels used during Score Matching with Langevin Dynamics.<br />    The sigma values are determined on a logarithmic scale from<br />    :obj:`sigma_max` to :obj:`sigma_min`, inclusive.<br /><br />    Args:<br />        sigma_min (float): The minimum value of sigma, corresponding to the<br />            lowest noise level.<br />        sigma_max (float): The maximum value of sigma, corresponding to the<br />            highest noise level.<br />        num_scales (int): The number of sigma values to generate, defining the<br />            granularity of the noise schedule.<br />        dtype (torch.dtype, optional): The output data type.<br />            (default: :obj:`None`)<br />        device (torch.device, optional): The output device.<br />            (default: :obj:`None`)<br />    &quot;&quot;&quot;<br />    return torch.linspace(<br />        math.log(sigma_max),<br />        math.log(sigma_min),<br />        num_scales,<br />        dtype=dtype,<br />        device=device,<br />    ).exp()</pre> <p>To utilize this function, it is necessary to specify the range and number of sigma values required for the schedule. The determination of these parameters depends on the specific needs of the model and the data. For example, small-scale graph generation with EDP-GNN in Niu 2020 uses six noise levels, while image generation models may require hundreds or thousands of noise levels. It often involves a process of experimentation and tuning to identify the most effective schedule for a given dataset and model architecture.</p> <p>The noise scheduler used in diffusion models, as detailed in the paper “Denoising Diffusion Probabilistic Models,” shares conceptual similarities with the one in Score Matching with Langevin Dynamics. However, it serves a different purpose and operates under different mechanics. In diffusion models, the scheduler generates a sequence of beta values for use in the forward diffusion process. Although this particular type of noise scheduler was not directly utilized in our project, we have implemented some variants to facilitate future diffusion-based graph models.</p> <h3>Edgewise Dense Prediction Graph Neural Network (EDP-GNN)</h3> <h4><strong>Multi-Channel GNN Layer</strong></h4> <p>The Multi-channel GNN layer, an extension of the Graph Isomorphism Network (GIN) layer, employs a novel approach to message passing. Its core idea involves simultaneously running message-passing algorithms over identical node features, but with different adjacency matrices. This process culminates in an output that is a concatenation of the outputs from all C channels, the number of channels is equal to the number of intermediate adjacency matrices. The formula for the m-th layer in this setup, tailored for a C-channel adjacency matrix input is below:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/437/1*jBSxC-S_hbpLPpPe1rVyeA.png"/></figure> <p>Notably, the Multi-channel GNN layer is a fundamental component of the final EDP-GNN model, enabling it to handle complex graph structures efficiently.<br/>The main parts of the implementations are message and forward functions.</p> <pre>    def message(self, x_j, edge_weight):<br />        return x_j * edge_weight<br /><br />    def forward(self, x: Union[Tensor, OptPairTensor], edge_indices: List[Adj],<br />                edge_weights: List[Tensor], size: Size = None) -&gt; Tensor:<br /><br />        ...<br />        edge_weights_cat = torch.cat(edge_weights, dim=-1)[:, None]<br />        # duplicate features to run over C channels<br />        <br />        ...<br />        # propagate_type: (x: OptPairTensor)<br />        out_cat = self.propagate(edge_index_cat, edge_weight=edge_weights_cat,<br />                                 x=x_cat, size=size)<br /><br />        x_r = x_cat[1]<br />        if x_r is not None:<br />            out_cat = out_cat + (1 + self.eps) * x_r  # N * C x F_in<br /><br />        # reshape to get concatenated features for each of C channels<br />        out_cat = out_cat.reshape((C, N, -1)).permute((1, 0, 2))<br />        out = out_cat.reshape((N, -1))<br /><br />        return self.nn(out)</pre> <h4>EDP-GNN layer</h4> <p>The EDP-GNN layer functions by processing an input consisting of a C-channel adjacency matrix accompanied by node features and outputs an updated C’-channel adjacency matrix also with node features.<br/>The computation of node features is conducted using the Multi-Channel GNN layer. Subsequently, a new C’-channel adjacency matrix is generated. This process involves utilizing the newly computed node features in conjunction with the original C-channel adjacency matrix. The method integrates these elements using concatenation and a Multilayer Perceptron (MLP) network. This approach underlines the layer’s efficiency in transforming and updating the graph structure. <br/>In this approach to modeling undirected graphs, it’s essential to maintain the symmetry of the predicted adjacency matrix. To achieve this, we sum the predicted matrix with its transposed version. This process ensures that the final adjacency matrix accurately reflects the undirected nature of the graph. For a more comprehensive understanding of this transformation, detailed formulas are provided below.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/280/1*BwL-eq_pz3lqnG3NYc6YBQ.png"/></figure> <h4>Final network architecture</h4> <p>The final network is represented by three layers as we can see on the image. Before being inputted into our EDP-GNN model, graphs undergo preprocessing. This involves using two-channel adjacency matrices: one channel for the original adjacency matrix and the other for its negated counterpart, with all entries inverted. Additionally, we initialize node features based on the weighted degrees of the nodes. This process ensures the graphs are optimally formatted for processing by the EDP-GNN model.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*KxzowzIrgtwOHsL5BSmV9A.jpeg"/><figcaption>High level of EDP-GNN with 3 layers</figcaption></figure> <p>Specifically, if we have node features <em>X</em> from the data, then the initial value for each node is as follows.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/199/1*iP3yLNhrdKckmhnav_TtnQ.png"/></figure> <p>A final point to note is that the dimensions of the inputs and outputs in our network are identical. This aspect is crucial because the network is designed to model the score function. Maintaining consistent dimensions ensures that the network accurately represents and processes the score function, which is vital for the model’s effectiveness.</p> <h4>Noise level conditioning</h4> <p>As previously mentioned, it’s necessary to condition each layer of our network on various noise levels. We achieve this noise conditioning by introducing a few learnable parameters, specifically by adding gains and biases. A conditional Multilayer Perceptron (MLP) layer is represented as:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/253/1*EX-fq9BgF1WcUL7FmMlLlQ.png"/></figure> <p>Where α and β are learnable parameters for each noise level σ.</p> <h3>Conclusion</h3> <p>In this post, we have explored the training and sampling process of a score-based graph generation method. This method leverages Graph Neural Networks (GNNs) to preserve the critical inductive bias of permutation invariance in the generated graphs. Furthermore, we demonstrated how the EDP-GNN network can be implemented using PyG utilities. If you want to delve deeper into the technical details, we encourage you to read the original paper available at <a href="https://arxiv.org/abs/2003.00638">https://arxiv.org/abs/2003.00638</a>. Thank you for joining us on this learning journey!</p> <h3>References</h3> <p>[1] C. Niu, Y. Song, J. Song, S. Zhao, A. Grover, and S. Ermon,<br/><a href="https://arxiv.org/abs/2003.00638">Permutation Invariant Graph Generation via Score-Based Generative Modeling</a>, (2020), AISTATS 2020<br/>[2] Song, Y. and Ermon, S. (2019). <a href="https://arxiv.org/abs/1907.05600">Generative modeling by estimating gradients of the data distribution</a>. arXiv preprint arXiv:1907.05600<br/>[3] Xu, K., Hu, W., Leskovec, J., and Jegelka, S. (2018a). <a href="https://arxiv.org/abs/1810.00826">How powerful are graph neural networks?</a> arXiv preprint arXiv:1810.00826<br/>[4] Jonathan Ho, Ajay Jain, Pieter Abbeel. Diffusion Probabilistic Models <a href="https://arxiv.org/abs/2006.11239">https://arxiv.org/abs/2006.11239</a><br/>[5] Yang Song. Diffusion and Score-Based Generative Models YouTube video. <a href="https://youtu.be/wMmqCMwuM2Q?si=64WID5_82zywM6_C">https://youtu.be/wMmqCMwuM2Q?si=64WID5_82zywM6_C</a></p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=e45c24d1ce89" width="1" height="1" alt=""/>&lt;hr&gt;&lt;p&gt;<a href="https://medium.com/stanford-cs224w/pyg-implementation-of-edp-gnn-generation-via-score-based-generative-modeling-e45c24d1ce89">PyG Implementation of EDP-GNN: Generation via Score-Based Generative Modeling</a> was originally published in <a href="https://medium.com/stanford-cs224w">Stanford CS224W: Machine Learning with Graphs</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</p>]]></content><author><name></name></author></entry><entry><title type="html">Live CNN Training Dashboard: Hyperparameters Tuning</title><link href="https://timashov.ai/blog/2020/live-cnn-training-dashboard-hyperparameters-tuning/" rel="alternate" type="text/html" title="Live CNN Training Dashboard: Hyperparameters Tuning"/><published>2020-11-14T14:17:06+00:00</published><updated>2020-11-14T14:17:06+00:00</updated><id>https://timashov.ai/blog/2020/live-cnn-training-dashboard-hyperparameters-tuning</id><content type="html" xml:base="https://timashov.ai/blog/2020/live-cnn-training-dashboard-hyperparameters-tuning/"><![CDATA[<h4><a href="https://towardsdatascience.com/tagged/hands-on-tutorials">Hands-on Tutorials</a></h4> <h3>Table of contents</h3> <ol><li><strong>Why we need to build a Live CNN Training Dashboard?</strong></li><li><strong>Introduction</strong></li><li><strong>Prerequisites</strong></li><li><strong>System description</strong></li><li><strong>How to create an environment and start training?</strong></li><li><strong>Conclusion</strong></li><li><strong>References</strong></li></ol> <h3>Why we need to build a Live CNN Training Dashboard?</h3> <p>When I studied in mathematical lyceum, my teacher taught me that the best way to understand something is to visualize it. For example, we had a wooden board, plasticine, and metal wire to be able to visualize stereometry problems. It helped a lot to develop visual thinking and skills in solving challenging tasks.</p> <p>I truly believe that real data scientists should understand algorithms and have a feeling on how to improve it if something works not fine. Especially in the area of deep learning. In my mind, the best way to develop these skills is to see how the model is trained, what happens when you change hyperparameters. This is the reason why I want to share how to build a simple dashboard for CNN live training with the opportunity to tune a few hyperparameters online.</p> <p>There is common knowledge that if we choose too big learning rate, we will see how our loss function explodes (our model will not converge); if we choose too small learning rate, the training process can last too long. What about dropout? There is an opinion that dropout reduces overfitting. I get to check everything myself even if I believe, because to know and to believe are different things.</p> <p>Below is the short demo of my dashboard. Red dots on loss function &amp; accuracy plots represent the training dataset, blue dots represent the test dataset.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/866/1*HhRidOhxiPrvrThYqqz3Pg.gif"/><figcaption>Image by the author</figcaption></figure> <h3>Introduction</h3> <p>Dashboard displays the following statistics:</p> <ul><li>loss function value in time;</li><li>accuracy in time;</li><li>distribution of activation maps values for the last step;</li><li>history of hyperparameters changes (table);</li></ul> <p>For this task, I am using AlexNet architecture to classify images on 10 classes: Alaskan malamute, baboon, echidna, giant panda, hippo, king penguin, llama, otter, red panda, and wombat. Images are downloaded from the ImageNet. I will not go into details in this post, but you can explore file <strong>get_dataset.py</strong>. During training, the following parameters can be tweaked:</p> <ul><li>optimizer;<br/>This parameter determines the algorithm we use to optimize our model. I use only Adam and SGD with Nesterov momentum. If you want to understand the optimization technique more, I encourage you to watch a video from Stanford <a href="https://www.youtube.com/watch?v=_JB0AO7QxSA">here</a>. There are many fantastic details about optimization.</li><li>learning rate;<br/>This parameter determines how fast we are moving down the slope when we are updating weights. For basic gradient descent formula for weights updates look like this: w := w — lr * dw.</li><li>weight decay;<br/>For our case it is simply L2 regularization: R(W) = SUM(W * W). It is considered that weight decay does not make a lot of sense in the context of CNN, but you can see it yourself how it works live. You can read some description of L1 and L2 regularization techniques <a href="https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c">here</a>.</li><li>dropout;<br/>Common regularization strategy for neural network. The idea is randomly set some neurons to zero on each training step. The hyperparameter is the probability to drop each neuron. Common value is 0.5 (50%). We can choose any integer value from 20 to 80. (in %) More details can be watched in the same video that I shared for optimizer.</li></ul> <p>Script can be easily changed to add additional functionality.</p> <h3>Prerequisites</h3> <p>I assume that you understand what is CNN and have basic knowledge of the following:</p> <ul><li>PostgreSQL (to store real-time data);</li><li>Dash (to build dashboard, <a href="https://plotly.com/dash/">https://plotly.com/dash/</a>);</li><li>PyTorch (to build CNN models);</li></ul> <h3>System description</h3> <p>There are four main parts of the system: <em>dataset, model, database, and dashboard/UI</em>. These parts interact with each other to successfully run the system. Firstly I will describe each of these parts and after that, I will give a short description of how they interact with each other.</p> <h4>Dataset</h4> <p>For this exercise, I use a dataset from the ImageNet that contains the following ten classes: <em>Alaskan malamute, baboon, echidna, giant panda, hippo, king penguin, llama, otter, red panda, and wombat. </em>To download all images from ImageNet, I can run python board.py from the following location: ../cnn_live_training.</p> <p>Firstly, I have to find classes ids and save them to some variable:</p> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/9f15dc13a20215b8bb39c30e5a55a92b/href">https://medium.com/media/9f15dc13a20215b8bb39c30e5a55a92b/href</a></iframe> <p>The ImageNet stores URLs to images. Some URLs/images might not exist anymore. To get these URLs based on class id, I use the following function:</p> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/e2e0b5d5a9bc2bf913270f868e053ede/href">https://medium.com/media/e2e0b5d5a9bc2bf913270f868e053ede/href</a></iframe> <p>To download all images I use a loop where I download image by image. Below is the function to download image by URL:</p> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/5e40c6ec93877779ada08f4bab23aae6/href">https://medium.com/media/5e40c6ec93877779ada08f4bab23aae6/href</a></iframe> <p>The full version of the code can be seen in the file <strong>get_dataset.py</strong>. You can easily change these classes to other classes or you can even change the ImageNet to your custom dataset.</p> <h4>Model</h4> <p>For the training, I am using by default the AlexNet architecture with Adam or SGD with Nesterov momentum optimizer. Optionally, the VGG16 can be chosen. Models can be imported either from the file <strong>models.py</strong> or from torchvision.models. The second option has the opportunity to use pre-trained weights. Dataset preparation happens in the file <strong>data_preparation.py</strong>. The training process happens in the file <strong>train.py</strong>.</p> <p>I don’t have the goal to explain in this article how to build a pipeline for training CNN that is why I am not going into detail in this part. But I am happy to recommend the amazing course CS231n from Stanford and particularly HW2(Q4), where you can learn step by step how to build this pipeline. This homework can be found <a href="https://cs231n.github.io/assignments2020/assignment2/">here</a>.</p> <h4>Database</h4> <p>Before running the system, we have to create <em>dl_playground</em> DB in PostgreSQL with the schema <em>cnn_live_training</em> that contains three following tables: <em>parameters, statistics, activations</em>.</p> <p><strong>parameters</strong><br/>This table contains <em>only one row</em> with current parameters for the training CNN model. When we change any parameters in our dashboard (file <strong>board.py</strong>), this data will be updated in the <em>parameters</em> SQL table. The table contains the following columns:</p> <ul><li>optimizer;<br/>Text data type. Can have two values: ‘Adam’ and ‘SGD+Nesterov’.</li><li>learning_rate;<br/>Double data type. The values are between 0 and 1 with the 0.00005 step.</li><li>weight_decay;<br/>Double data type. The values are between 0 and 1 with the 0.05 step.</li><li>dropout;<br/>Integer data type. The values are between 20 and 80. (It is assumed that the values are in %.)</li><li>dt_updates;<br/>Timestamp data type. Indicates date and time when data was modified.</li><li>stop_train;<br/>Boolean data type. Indicates if we have to stop training.</li></ul> <p><strong>statistics<br/></strong> This table contains statistics of the training process. Data is updated every --n-print step. The table contains the following columns:</p> <ul><li>dt_started;<br/>Timestamp data type. Indicates when current training was started.</li><li>model_name;<br/>Text data type. In this case, it can be only ‘MyAlexNet’.</li><li>epoch;<br/>Integer data type. Indicates the number of training epochs.</li><li>step;<br/>Integer data type. Indicates the number of training steps.</li><li>optimizer;<br/>Text data type. Can have two values: ‘Adam’ and ‘SGD+Nesterov’.</li><li>learning_rate;<br/>Double data type. The values are between 0 and 1 with the 0.00005 step.</li><li>weight_decay;<br/>Double data type. The values are between 0 and 1 with the 0.05 step.</li><li>dropout;<br/>Integer data type. The values are between 20 and 80.</li><li>dt;<br/>Timestamp data type. Indicates date and time when data was modified.</li><li>train_loss;<br/>Double data type. The value of loss function for the training dataset on the last step.</li><li>train_accuracy;<br/>Double data type. The value of accuracy for the training dataset on the last step.</li><li>validate_loss;<br/>Double data type. The value of loss function for the validation dataset on the last step.</li><li>validate_accuracy;<br/>Double data type. The value of accuracy for the validation dataset on the last step.</li></ul> <p><strong>activations</strong><br/>This table contains the current distribution of weights in activation maps for all convolutional and fully connected layers. The table contains the following columns:</p> <ul><li>nn_part;<br/>Text data type. Can be either ‘features’ or ‘classifier’.</li><li>layer_type;<br/>Text data type. Can be either ‘conv’ or ‘fc’.</li><li>number;<br/>Integer data type. Indicates the layer number in a ‘nn’ part.</li><li>weights;<br/>Double[] data type. Indicates average values of weights in bins.</li><li>num_weights;<br/>Integer[] data type. Indicates numbers of values in bins.</li></ul> <h4>Dashboard/UI</h4> <p>The dashboard consists of three main blocks: <em>control panel, loss function &amp; accuracy, and activation maps (distribution)</em>. These blocks are built using dash containers.</p> <p><strong>Control panel</strong> contains filters of parameters and “submit parameters” button that can be used to send chosen parameters to described above table “parameters”.<br/>There are four filters: optimizer, learning rate, weight decay, and dropout.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*gO3qCjmZ2W_t53D2BY5U9g.png"/><figcaption>Image by the author</figcaption></figure> <p>Below is the script, how to create an optimizer filter (other filters are similar):</p> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/4f8d477162beffe5461cdbbcc80f0906/href">https://medium.com/media/4f8d477162beffe5461cdbbcc80f0906/href</a></iframe> <p>After that I create a container that contains all four filters:</p> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/664f7649c8dde620e428b38602be1f62/href">https://medium.com/media/664f7649c8dde620e428b38602be1f62/href</a></iframe> <p>How to create other parts of the control panel can be found in the file <strong>board.py</strong>.</p> <p><strong>Loss function &amp; Accuracy</strong> contains a table with the history of used parameters and two plots with train/test loss function and accuracy values in time. Data is updated every one second (time interval can be changed) automatically.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*pnOp9JjL1Si19I9VCmsd-Q.png"/><figcaption>Image by the author</figcaption></figure> <p>Below is the script on how to create a table and button to stop training in the dashboard (I replaced real styles with short names for reading convenience):</p> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/3789925ae6f251b225c7c20a2732e11f/href">https://medium.com/media/3789925ae6f251b225c7c20a2732e11f/href</a></iframe> <p>Script to create plot template can be seen below:</p> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/7297eb93fdf91a4436782acec545f1d1/href">https://medium.com/media/7297eb93fdf91a4436782acec545f1d1/href</a></iframe> <p>Values are uploaded dynamically from PostgreSQL using callbacks (I provide only template for reading convenience):</p> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/96ca5ddf33d265257bd1131dc18c5246/href">https://medium.com/media/96ca5ddf33d265257bd1131dc18c5246/href</a></iframe> <p>I need to use a callback here because I want to update the plot and the table every 1 second. So, I have to use this variable as an input.</p> <p><strong>Activation maps (distribution)</strong> contains plots with distribution of activation map for each layer for the last step. Data is updated every one second (time interval can be changed) automatically.</p> <p>The activations of the first two layers look similar to a normal distribution with the mean value in 0. The reason for this is for the first two layers we apply normalization. To understand more, I encourage you to watch a lecture from Stanford <a href="https://www.youtube.com/watch?v=wEoyxE0GP2M">here</a>.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Hp19LuAebQOoa7F8wZL9dQ.png"/><figcaption>Image by the author</figcaption></figure> <p>Below is the script to create a container with the plots. It is similar to the previous container with loss function and accuracy plots:</p> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/6cc47a3ce1631f15e01eef05b1e11308/href">https://medium.com/media/6cc47a3ce1631f15e01eef05b1e11308/href</a></iframe> <p>The callback for the activation maps is similar to the “loss function &amp; accuracy”:</p> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/bd88a63b7ba7f0cc176093907b5265cd/href">https://medium.com/media/bd88a63b7ba7f0cc176093907b5265cd/href</a></iframe> <h4>How everything works</h4> <p>It’s time to wrap everything up. To recall back, my goal is to train CNN live and being able to control this process by changing hyperparameters. So how does it happen? I have a dashboard where we can see the progress of the CNN training and where we have some filters that we can choose and activate by pushing the button “Submit parameters”.</p> <p>What happens after that? All these parameters are sent to the table <em>parameters</em> in my database in PostgreSQL, using callback in the file <strong>board.py</strong> and function <em>update_params</em>:</p> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/3167abb99f196edc04396152c1b451bf/href">https://medium.com/media/3167abb99f196edc04396152c1b451bf/href</a></iframe> <p>At the same time, the script <strong>train.py</strong> connects to a database at the end of each training step, seeking to update the optimizer if parameters get updated:</p> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/7ed8b471a893e37137e9db49c348ab03/href">https://medium.com/media/7ed8b471a893e37137e9db49c348ab03/href</a></iframe> <p>Every <em>n_step</em> step data from training is saved to <em>statistics</em> and <em>activations</em> tables in database in PostgreSQL:</p> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/8a5190d919b9f9d5182aca2b93e1140a/href">https://medium.com/media/8a5190d919b9f9d5182aca2b93e1140a/href</a></iframe> <p>And this data simultaneously displayed in the dashboard because the script <strong>board.py</strong> every 1 sec. connects to the same tables:</p> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/d86dbc3de1f22ea60d0dd7fd5a9d4c88/href">https://medium.com/media/d86dbc3de1f22ea60d0dd7fd5a9d4c88/href</a></iframe> <p>All parameters are displayed in the table by extracting this information from the table :</p> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/d8ea4b68cdc1c63bf63ebfa0aeac795b/href">https://medium.com/media/d8ea4b68cdc1c63bf63ebfa0aeac795b/href</a></iframe> <p>If we want to stop training beforehand, we can push the button “Stop Training” below the table. After pushing the button, the callback will change the variable <em>stop_train</em> from <em>False</em> to <em>True</em> in the <em>parameters</em> table in my database:</p> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/cc7ed36586b8e1730acef94864e944a0/href">https://medium.com/media/cc7ed36586b8e1730acef94864e944a0/href</a></iframe> <p>At the same time, the script <strong>train.py</strong> check this parameter every training step and if it is <em>True</em>, training will be interrupted.</p> <p>Without practical recommendations on what parameters to use to start training, this post will not be complete. If you want to see that everything works, but don’t have time for experiments, you can start from the following parameters:</p> <ul><li>optimizer: Adam;</li><li>learning rate: 0.0003;</li><li>weight decay: 0;</li><li>dropout: 50%;</li></ul> <p>If you want to see how the model explodes, just increase the learning rate to 0.01. Good luck with your experiments.</p> <h3>How to create an environment and start training?</h3> <h4>Virtual environment setting up</h4> <p>I will give a short description for Ubuntu, using a virtual environment (<em>venv</em>).</p> <ol><li>Install Python 3.8: sudo apt install python3.8-minimal</li><li>Install virtual environment with Python 3.8: sudo apt-get install python3.8-venv</li><li>Create virtual environment: run from cnn_live_training folder: python3.8 -m venv venv</li><li>Activate environment: source venv/bin/activate</li><li>Install required packages in the virtual environment: <br/>pip install -r requirements.txt</li></ol> <h4>Collect dataset</h4> <p>Run from the ../cnn_live_training command python get_dataset.py</p> <h4>Start training</h4> <p>Run from the ../cnn_live_training folder two following commands</p> <pre>python board.py<br />python train.py</pre> <h3>Conclusion</h3> <p>In this story, I wanted to share my idea on how to nurture the feeling of training CNN. From one side, the idea is simple: build a training pipeline, create a dashboard and connect them using a database. But there are many annoying details that not possible to put in one small story. All script and additional details can be found in my <a href="https://github.com/atimashov/cnn_live_training">git repository</a>.</p> <p>If this post makes someone interested and give additional knowledge, I will become slightly happier because it means that I reached my goal. I will appreciate any comments, constructive criticism, or questions, feel free to leave your feedback below or you can reach me via <a href="https://www.linkedin.com/in/alexander-timashov/">LinkedIn</a>.</p> <h3>References</h3> <p>[1] L. Fei-Fei, R. Krishna and D. Xu, <a href="http://cs231n.stanford.edu/">CS231n: Convolutional Neural Networks for Visual Recognition</a> (2020), Stanford University</p> <p>[2] A. Krizhevsky, I. Sutskever and G. E. Hinton, <a href="https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf">ImageNet Classification with Deep Convolutional Neural Networks</a> (2012), NeurIPS 2012</p> <p>[3] A. Nagpal, <a href="https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c">L1 and L2 Regularization Methods</a> (2017), Towards Data Science</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=6b3382d9e44f" width="1" height="1" alt=""/>&lt;hr&gt;&lt;p&gt;<a href="https://medium.com/data-science/live-cnn-training-dashboard-hyperparameters-tuning-6b3382d9e44f">Live CNN Training Dashboard: Hyperparameters Tuning</a> was originally published in <a href="https://medium.com/data-science">TDS Archive</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</p>]]></content><author><name></name></author></entry></feed>