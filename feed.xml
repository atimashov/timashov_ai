<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://timashov.ai/feed.xml" rel="self" type="application/atom+xml"/><link href="https://timashov.ai/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-05-12T16:26:52+00:00</updated><id>https://timashov.ai/feed.xml</id><title type="html">Aleksandr Timashov</title><subtitle>The personal page of Aleksandr Timashov </subtitle><entry><title type="html">Backpropagation: From Intuition to FLOPs</title><link href="https://timashov.ai/blog/2025/backward-compute/" rel="alternate" type="text/html" title="Backpropagation: From Intuition to FLOPs"/><published>2025-04-28T09:00:00+00:00</published><updated>2025-04-28T09:00:00+00:00</updated><id>https://timashov.ai/blog/2025/backward%20compute</id><content type="html" xml:base="https://timashov.ai/blog/2025/backward-compute/"><![CDATA[<p>This post dives into what happens during the backward pass of a neural network. We’ll start with a simple scalar example and build up to full matrix-based gradient derivations — showing not only how gradients are computed, but also how to count <strong>FLOPs</strong> along the way.</p> <h2 id="introduction">Introduction</h2> <p>In the previous <a href="timashov.ai/blog/2025/compute/">post</a> I covered how to construct tensors and pass them through operations - this is known as the “forward pass”.</p> <p>The next step in deep learning is computing <strong>gradients</strong> and making an optimizer step - the <strong>backward pass</strong>.</p> <p>Let’s look at a simple example: $y=0.5 \times (x \times w-5)^2$.<br/> <strong>Forward pass:</strong> compute loss</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>x = torch.Tensor([1.0, 2.0, 3.0]) # we don't need gradients
w = torch.Tensor([1.0, 1.0, 1.0], requres_grad = True) # we need gradients
pred_y = x @ w
loss  = 0.5 * (pred_y - 5).pow(2)
</code></pre></div></div> <p><strong>Backward pass:</strong> compute gradients</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>loss.backward()
assert loss.grad is None
assert pred_y.grad is None
assert x.grad is None
assert torch.equal(w.grad, torch.tensor(1.0, 2.0, 3.0))
</code></pre></div></div> <hr/> <h2 id="matrix-gradients-by-hand-and-backward-flops">Matrix Gradients by Hand and Backward FLOPs</h2> <p>Let’s consider a more realistic setup. We have</p> <ul> <li>input matrix <strong><em>X</em></strong> with dimensions $B \times D$</li> <li>hidden layer(1) <strong><em>W1</em></strong> with dimensions $D \times D$</li> <li>hidden layer(2) <strong><em>W2</em></strong> with dimensions $D \times C$ (outputs <strong><em>C</em></strong> classes)</li> </ul> <p>The model is</p> <ul> <li>$H_1 = XW_1$ (the dimensions: \((B \times D) \ @ \ (D \times D) \to B \times D\))</li> <li>$H_2 = H_1W_2$ (the dimensions: \((B \times D) \ @ \ (D \times C) \to B \times C\))</li> <li>$\text{loss} = \text{mean}(H_2^2)$</li> </ul> <p>To perform a <strong>backward step</strong>, we will need <strong>gradients with respect to all weights</strong>: $\frac{\text{dloss}}{\text{dw}_1}$ and $\frac{\text{dloss}}{\text{dw}_2}$.</p> <p>Let’s visualise matrices, this helps to build intuition for how gradients are calculated step by step.</p> \[X = \begin{bmatrix} x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1D} \\ x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2D} \\ \cdots &amp; \cdots &amp; \cdots &amp; \cdots \\ x_{B1} &amp; x_{B2} &amp; \cdots &amp; x_{BD} \\ \end{bmatrix} \qquad W_1 = \begin{bmatrix} w^{(1)}_{11} &amp; w^{(1)}_{12} &amp; \cdots &amp; w^{(1)}_{1D} \\ w^{(1)}_{21} &amp; w^{(1)}_{22} &amp; \cdots &amp; w^{(1)}_{2D} \\ \cdots &amp; \cdots &amp; \cdots &amp; \cdots \\ w^{(1)}_{D1} &amp; w^{(1)}_{D2} &amp; \cdots &amp; w^{(1)}_{DD} \\ \end{bmatrix} \qquad W_2 = \begin{bmatrix} w^{(2)}_{11} &amp; w^{(2)}_{12} &amp; \cdots &amp; w^{(2)}_{1C} \\ w^{(2)}_{21} &amp; w^{(2)}_{22} &amp; \cdots &amp; w^{(2)}_{2C} \\ \cdots &amp; \cdots &amp; \cdots &amp; \cdots \\ w^{(2)}_{D1} &amp; w^{(2)}_{D2} &amp; \cdots &amp; w^{(2)}_{DC} \\ \end{bmatrix}\] <h3 id="forward-pass">Forward pass</h3> <p>We’ll walk through each layer’s output to build a concrete understanding of how activations are computed.</p> <p><strong>Step1:</strong> Compute <strong><em>H₁</em></strong></p> \[H_1 = XW_1 = \begin{bmatrix} \sum_{k=1}^D x_{1k}w^{(1)}_{k1} &amp; \sum_{k=1}^D x_{1k}w^{(1)}_{k2} &amp; \cdots &amp; \sum_{k=1}^D x_{1k}w^{(1)}_{kD} \\ \sum_{k=1}^D x_{2k}w^{(1)}_{k1} &amp; \sum_{k=1}^D x_{2k}w^{(1)}_{k2} &amp; \cdots &amp; \sum_{k=1}^D x_{2k}w^{(1)}_{kD} \\ \cdots &amp; \cdots &amp; \cdots &amp; \cdots \\ \sum_{k=1}^D x_{Bk}w^{(1)}_{k1} &amp; \sum_{k=1}^D x_{Bk}w^{(1)}_{k2} &amp; \cdots &amp; \sum_{k=1}^D x_{Bk}w^{(1)}_{kD} \end{bmatrix}\] <p><strong>Step2:</strong> Compute <strong><em>H₂</em></strong></p> \[H_2 = H_1W_2 = \begin{bmatrix} \sum_{k=1}^D h^{(1)}_{1k}w^{(2)}_{k1} &amp; \sum_{k=1}^D h^{(1)}_{1k}w^{(2)}_{k2} &amp; \cdots &amp; \sum_{k=1}^D h^{(1)}_{1k}w^{(2)}_{kC} \\ \sum_{k=1}^D h^{(1)}_{2k}w^{(2)}_{k1} &amp; \sum_{k=1}^D h^{(1)}_{2k}w^{(2)}_{k2} &amp; \cdots &amp; \sum_{k=1}^D h^{(1)}_{2k}w^{(2)}_{kC} \\ \cdots &amp; \cdots &amp; \cdots &amp; \cdots \\ \sum_{k=1}^D h^{(1)}_{Bk}w^{(2)}_{k1} &amp; \sum_{k=1}^D h^{(1)}_{Bk}w^{(2)}_{k2} &amp; \cdots &amp; \sum_{k=1}^D h^{(1)}_{Bk}w^{(2)}_{kC} \end{bmatrix}\] <p>More generally, element $h^{(2)}_{ij} = \sum _{k=1}^D h^{(1)} _{ik} w^{(2)} _{kj}$.</p> <p><strong>Step3:</strong> Compute the final loss<br/> \(\text{loss} = \frac{1}{BC}\sum_{i=1}^B \sum_{j=1}^C (h^{(2)}_{ij})^2\)</p> <p>Forward FLOPs (excluding loss): <code class="language-plaintext highlighter-rouge">num_forward_flops = 2 * B * D * D + 2 * B * D * C</code>.</p> <p>Now that we’ve computed the forward activations, it’s time to trace gradients backward using the chain rule.</p> <h3 id="backward-pass">Backward pass</h3> <p>To compute gradients, we use the <strong>chain rule</strong> - “a formula that expresses the derivative of the composition of two differentiable functions” (<a href="https://en.wikipedia.org/wiki/Chain_rule">wiki</a>). In Deep Learning it is often more intuitive to write it using Leibniz notation: if $z$ depends on $y$ and $y$ depends on $x$, then $z$ depends on $x$ and we can write it as</p> \[\frac{dz}{dx} = \frac{dz}{dy} \frac{dy}{dx}\] <h4 id="gradient-with-respect-to-h"><u>Gradient with respect to H₂</u></h4> <p>Now that we’ve defined the loss, the first step in the backward pass is to compute the gradient of the loss with respect to the model’s output, <strong><em>H₂</em></strong>:</p> \[\frac{\text{dloss}}{dh^{(2)}_{ij}}=\frac{1}{BC}\cdot 2 \cdot dh^{(2)}_{ij} \quad \text{(Only one tem of the sum depends on } h^{(2)}_{ij} \text{)}\] <p>Or in matrix form:</p> \[\frac{\text{dloss}}{dH_2}=\frac{2}{BC}H_2\] <h4 id="gradient-with-respect-to-w"><u>Gradient with respect to W₂</u></h4> <p>With \(\frac{\text{dloss}}{dH_2}\) computed, we can calculate the gradient w.r.t. the second layer weights, <strong><em>W₂</em></strong>, using the chain rule.<br/> We start with:</p> \[\frac{\text{dloss}}{dw^{(2)}_{ij}} = \sum_{i^{'}=1}^B \sum_{j^{'}=1}^C \frac{\text{dloss}}{dh^{(2)}_{i^{'}j^{'}}} \cdot \frac{dh^{(2)}_{i^{'}j^{'}}}{dw^{(2)}_{ij}}\] <p>The <strong>key insight</strong> is that the sum includes many terms that are zero - \(h^{(2)}_{i^{'}j^{'}}=\sum_{k=1}^{D} h^{(1)}_{i^{'}k}w^{(2)}_{kj^{'}}\) only depends on \(w^{(2)}_{ij}\) when \(j^{'}=j\).</p> <p>Let’s derive it more formally:</p> \[\frac{dh^{(2)}_{i^{'}j^{'}}}{dw^{(2)}_{ij}} = \begin{cases} h_{i^{'}i} &amp; \text{if } j^{'} = j \\ 0 &amp; \text{if } j^{'} \neq j \end{cases} \quad \Rightarrow \quad \frac{\text{dloss}}{dw^{(2)}_{ij}} = \sum_{k=1}^{B} \frac{\text{dloss}}{dh^{(2)}_{kj}} \cdot h^{(1)}_{ki} = \sum_{k=1}^{B} (h^{(1)})^T_{ik} \cdot \frac{\text{dloss}}{dh^{(2)}_{kj}}\] <p>In matrix form it will look this way:</p> \[\frac{\text{dloss}}{dW_2} = H_1^T \frac{\text{dloss}}{dH_2} \text{ (dimensions: } D \times C\text{)}\] <p>This is a matrix multiplication between a \(D \times B\) and a \(B \times C\) matrix - resulting in a \(D \times C\) output.<br/> The number of backward FLOPs so far: <code class="language-plaintext highlighter-rouge">num_backward_flops = 2 * B * D * C</code>.</p> <h4 id="gradient-with-respect-to-h-1"><u>Gradient with respect to H₁</u></h4> <p>Next, we need to backpropagate through <strong><em>W₂</em></strong> into the hidden activations <strong><em>H₁</em></strong>. This is needed to continue the chain toward <strong><em>W₁</em></strong>. Following the same idea, we can get:</p> \[\frac{\text{dloss}}{dH_1} = \frac{\text{dloss}}{dH_2}W_2^T \text{ (dimensions: } B \times D\text{)}\] <p>The number of backward FLOPs so far: <code class="language-plaintext highlighter-rouge">num_backward_flops = 4 * B * D * C = 2 * B * D * C + 2 * B * D * C</code>.</p> <h4 id="gradient-with-respect-to-w-1"><u>Gradient with respect to W₁</u></h4> <p>Now we compute gradients for the first layer weights, \(W_1\) - this completes the parameter gradient path for training.</p> \[\frac{\text{dloss}}{dw^{(1)}_{ij}} = \sum_{i^{'}=1}^B \sum_{j^{'}=1}^D \frac{\text{dloss}}{dh^{(1)}_{i^{'}j^{'}}} \cdot \frac{dh^{(1)}_{i^{'}j^{'}}}{dw^{(1)}_{ij}} = \sum_{k=1}^D \frac{\text{dloss}}{dh^{(1)}_{kj}} \cdot \frac{dh^{(1)}_{kj}}{dw^{(1)}_{ij}} = \sum_{k=1}^D \frac{\text{dloss}}{dh^{(1)}_{kj}} \cdot x_{ki}\] <p>So, in matrix form:</p> \[\frac{\text{dloss}}{dW_1} = \frac{\text{dloss}}{dH_1^T}X \text{ (the dimensions: } D \times D\text{)}\] <p>Updated backwards FLOPs: <code class="language-plaintext highlighter-rouge">num_backward_flops = 4 * B * D * C + 2 * B * D * D</code>.</p> <h4 id="gradient-with-respect-to-x"><u>Gradient with respect to X</u></h4> <p>We can also compute gradients w.r.t. the input $X$ — although this <strong>isn’t needed during training</strong>, it’s useful for things like input sensitivity or adversarial attacks. Following the same chain rule logic:</p> \[\frac{\text{dloss}}{dX} = \frac{\text{dloss}}{dH_1}W_1^T \text{ (the dimensions: } B \times D\text{)}\] <p>The number of backward FLOPs: <code class="language-plaintext highlighter-rouge">num_backward_flops = 4 * B * D * C + 4 * B * D * D</code>. This is exactly $2\times$ <strong>the number of FLOPs</strong> compared to the forward pass.</p> <hr/> <h2 id="conclusion">Conclusion</h2> <ul> <li><strong>Understanding the math</strong> behind helps in debugging, optimizing, and designing new architectures.</li> <li>The <strong>backward pass</strong> is built on the <strong>chain rule</strong>, propagated layer by layer.</li> <li>Backward pass requires <strong>~2x the GLOPs</strong> of the forward pass. &lt;!– Understanding data types is the foundation of building efficient AI systems.<br/> It affects not only how we design models but also how fast and how large they can be.</li> </ul> <p>In future posts, I’ll dive deeper into <strong>resource accounting</strong> — covering both <strong>memory</strong> and <strong>FLOPS</strong>. For memory, I’ll go beyond inputs to include gradients, intermediate activations, and other internal components of deep learning models. –&gt;</p>]]></content><author><name></name></author><category term="cs336"/><category term="dl-basics,"/><category term="lecture-1"/><summary type="html"><![CDATA[gradient calculation, flow and FLOPs]]></summary></entry><entry><title type="html">DL Under the Hood: Tensors, Views, and FLOPs</title><link href="https://timashov.ai/blog/2025/compute/" rel="alternate" type="text/html" title="DL Under the Hood: Tensors, Views, and FLOPs"/><published>2025-04-27T09:00:00+00:00</published><updated>2025-04-27T09:00:00+00:00</updated><id>https://timashov.ai/blog/2025/compute</id><content type="html" xml:base="https://timashov.ai/blog/2025/compute/"><![CDATA[<p>This post explores what happens under the hood when we create and operate on tensors in PyTorch. From device placement to memory sharing and floating-point operation (FLOP) counts, we’ll see how tensor computations map to real hardware usage and performance.</p> <hr/> <h2 id="introduction-tensors-and-memory-cpu-vs-gpu">Introduction: Tensors and Memory (CPU vs GPU)</h2> <p>By default, tensors are stored in <strong>CPU memory:</strong> <code class="language-plaintext highlighter-rouge">x=torch.zeros(32, 32)</code>.<br/> if GPU is available, we <strong>can move</strong> tensor there: <code class="language-plaintext highlighter-rouge">x.to('cuda:0')</code>.<br/> or create it directly on GPU: <code class="language-plaintext highlighter-rouge">x=torch.zeros(32, 32, device = 'cuda:0')</code>.</p> <p><img src="/assets/img/compute/cpu_gpu.png" alt="Img.1: CPU-GPU data processing" style="width:100%;"/></p> <p><strong>Notes:</strong></p> <ul> <li>To leverage <strong>GPU acceleration</strong>, we must ensure tensors are on the GPU. This is key to unlocking its massive parallelism.</li> <li>Always know where your tensor lives: <code class="language-plaintext highlighter-rouge">assert x.device == torch.device('cpu')</code>.</li> </ul> <p><strong>Useful PyTorch Utilities:</strong></p> <ul> <li><code class="language-plaintext highlighter-rouge">torch.cuda.is_available()</code> - check if GPU is available, returns True or False.</li> <li><code class="language-plaintext highlighter-rouge">torch.cuda.get_device_properties(i)</code> - inspect GPU “i” specs, returns information about GPU number “i”.</li> <li><code class="language-plaintext highlighter-rouge">torch.cuda.memory_allocated()</code> - returns allocated memory, great for debugging memory used by tensors.</li> <li>Check if two tensors share the same underlying memory: <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def same_storage(x: torch.Tensor, y: torch.Tensor): -&gt; bool
  x.untyped_storage().data_ptr() == y.untyped_storage().data_ptr()
</code></pre></div> </div> </li> </ul> <hr/> <h2 id="tensor-storage-and-views">Tensor Storage and Views</h2> <p>Tensors in PyTorch are <strong>pointers</strong> into allocated memory. Each tensor stores metadata that describes how to access any specific element in that memory block.</p> <p><img src="/assets/img/compute/tensor_storage.png" alt="Img.2: Underlying Storage for Tensors" style="width:100%;"/></p> <p>This is <strong>important</strong> because multiple tensors can share the same underlying storage — even if they represent different shapes or slices. These are called <strong>views</strong>, and they do <strong>not allocate new memory</strong>.</p> <p><strong>Examples:</strong></p> <ul> <li>slicing <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>y = x[0]
assert same_storage(y, x[0])
</code></pre></div> </div> </li> <li>taking a view <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>x = torch.randn(3,2)
y = x.view(2,3)
assert same_storage(y, x)
</code></pre></div> </div> </li> <li>transposing <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>x = torch.randn(3,2)
y = x.transpose(0,1)
assert same_storage(y, x)
</code></pre></div> </div> </li> </ul> <p>If we modify a tensor through one of its views, the change will be visible through all other — since they share the same memory.</p> <p><strong>Notes:</strong></p> <ul> <li> <p>Some views are <strong>non-contiguous</strong> entries. It means <strong>further views</strong> are not <strong>possible</strong>.<br/> Contiguous means when we iterate over the tensor, we’re reading memory <strong>sequentially</strong>, not jumping or skipping around.<br/> Some operations (like <code class="language-plaintext highlighter-rouge">.view()</code>) require the tensor to be contiguous. If it’s not, you can fix that using <code class="language-plaintext highlighter-rouge">.contiguous()</code>, which copies the data into a contiguous layout.</p> </li> <li> <p>Elementwise operations <strong>create new tensors</strong>: <code class="language-plaintext highlighter-rouge">.triu()</code> (helps create a causal attention mask), <code class="language-plaintext highlighter-rouge">.pow(2)</code>, <code class="language-plaintext highlighter-rouge">.sqrt()</code>, <code class="language-plaintext highlighter-rouge">.rsqr()</code>, etc.</p> </li> </ul> <hr/> <h2 id="matrix-multiplication-in-pytorch">Matrix Multiplication in PyTorch</h2> <p>From linear algebra we remember that <strong>matrix multiplication</strong> requires <strong>the inner dimensions match</strong> — the number of columns in the first matrix must be equal the number of rows in the second.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>x = torch.ones(16, 32)
w = torch.ones(32, 2)
y = x @ w
assert y.size() == torch.Size([16,2])
</code></pre></div></div> <p>In deep learning, we <strong>often perform</strong> the same operation across a <strong>batch of inputs</strong>, and for token sequences in NLP:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>B, L = 128, 1024
x = torch.ones(B, L, 16, 32)
w = torch.ones(32, 2)
y = x @ w
assert y.size() == torch.Size([B, L, 16,2])
</code></pre></div></div> <hr/> <h2 id="named-dimensions-and-einstein-notation">Named Dimensions and Einstein Notation</h2> <p>Traditional PyTorch code often relies on <strong>positional indexing</strong>, which becomes error-prone when tensors have many dimensions:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>B, S, H = 2, 2, 3
x = torch.ones(B, H, S) # batch, sequence, hidden
y = torch.ones(B, H, S) # batch, sequence, hidden
z = x @ y.transpose(-2,-1) # batch, sequence, sequence
</code></pre></div></div> <p>But what exactly are -2 and -1? We have to mentally track the meaning of each axis — and that’s fragile.</p> <p><strong>Proposal:</strong> Use <strong>named dimensions</strong> instead of relying on raw indices. This is where <strong>jaxtyping</strong> and <strong>einops</strong> shine:</p> <ul> <li>Classic style: <code class="language-plaintext highlighter-rouge">x = torch.randn(2, 2, 1, 3) # batch seq heads hidden</code></li> <li>Named style: <code class="language-plaintext highlighter-rouge">x: Float[torch.Tensor, "batch seq heads hidden"] = torch.randn(2, 2, 1, 3)</code></li> </ul> <p><strong>Note:</strong> We are <strong>just documenting</strong>, not enforcing. Enforcement is possible, but requires raising check explicitly.</p> <p><strong>Einstein Summation with einsum</strong><br/> Einstein notation (via <code class="language-plaintext highlighter-rouge">einops.einsum</code>) extends matrix multiplication with intuitive dimension bookkeeping. We specify input dims, and what dims remain — everything else is summed over.</p> <ul> <li>Before<br/> <code class="language-plaintext highlighter-rouge">z = x @ y.transpose(-2, -1)</code></li> <li>After<br/> <code class="language-plaintext highlighter-rouge">z = einsum(x, y, "batch seq1 hidden, batch seq2 hidden -&gt; batch, seq1, seq2")</code></li> <li>Moregenerally, over multiple dimensions<br/> <code class="language-plaintext highlighter-rouge">z = einsum(x, y, "... seq1 hidden, ... seq2 hidden -&gt; ... seq1, seq2")</code></li> </ul> <p>Other beneficial <strong>Einsum Patterns</strong>:</p> <ul> <li>Reduce (e.g. sum over hidden dim): <code class="language-plaintext highlighter-rouge">y = reduce(x, "... hidden -&gt; ...", "sum")</code></li> <li>Rearrange (e.g. split/merge heads): <code class="language-plaintext highlighter-rouge">x = rearrange(x, "... (heads hidden1) -&gt; ... heads hidden1", heads = 2)</code><br/> This replaces the need for <code class="language-plaintext highlighter-rouge">view</code>, <code class="language-plaintext highlighter-rouge">permute</code>, and <code class="language-plaintext highlighter-rouge">reshape</code> — with explicit, readable transformations.</li> </ul> <p><strong>Notes:</strong></p> <ul> <li>jaxtyping <a href="https://einops.rocks/1-einops-basics/">tutorial</a></li> <li>einops <a href="https://docs.kidger.site/jaxtyping/">tutorial</a></li> </ul> <hr/> <h2 id="measuring-compute-flops-and-performance">Measuring Compute: FLOPs and Performance</h2> <p><strong>Definition:</strong> A floating point operation (<strong>FLOP</strong>) is a basic aithmetic operation like addition (<code class="language-plaintext highlighter-rouge">a + b</code>) or multiplication (<code class="language-plaintext highlighter-rouge">a * b</code>).<br/> <strong>Notes:</strong> In these notes (and CS336 class), we distinguish:</p> <ul> <li><strong>FLOPs:</strong> total number of floating point operations.</li> <li><strong>FLOP/s:</strong> floating point operations per second (speed of hardware).</li> </ul> <p><strong>Useful numbers:</strong></p> <ul> <li>Training GPT-3 (2020) took <strong>3.14e23</strong> FLOPs</li> <li>Training GPT-4 (2023) took (rumoured) <strong>2e25</strong> FLOPs (<strong>~64x</strong> GPT-3)</li> <li>US executive order (revoked in 2025): any foundation model trained with <strong>1e26</strong> FLOPs must be reported to the government</li> <li>A100 has a peak performance of <strong>312e12</strong> FLOP/s (<strong>312</strong> teraFLOP/s)</li> <li>H100 has a peak performance of <strong>1979e12</strong> FLOP/s with sparsity, <strong>~50%</strong> - -without</li> <li>RTX5090: [specs TBD]</li> </ul> <p><strong>Note:</strong> Actual performance depends on data types used (e.g., float32 vs bfloat16) and workload structure.<br/> See NVIDIA spec sheets for precise values.<br/> <strong>Example Calculation</strong>: To train <strong>GPT-4</strong> model it is required <code class="language-plaintext highlighter-rouge">2e25 / (1979e12 * 0.5) / (60 * 60 * 24) ~ 2.34e5</code> <strong>H100 days</strong>. ([check])</p> <p><strong>Matrix Multiplication: FLOPs Count</strong><br/> Let’s recall how matrix multiplication works:</p> \[\begin{bmatrix} a_{11} &amp; a_{12} &amp; a_{13} \\ a_{21} &amp; a_{22} &amp; a_{23} \end{bmatrix} \begin{bmatrix} b_{11} &amp; b_{12}\\ b_{21} &amp; b_{22}\\ b_{31} &amp; b_{32} \end{bmatrix}= \begin{bmatrix} a_{11}b_{11} + a_{12}b_{21} + a_{13}b_{31} &amp; a_{11}b_{12} + a_{12}b_{22} + a_{13}b_{32}\\ a_{21}b_{11} + a_{22}b_{21} + a_{23}b_{31} &amp; a_{21}b_{12} + a_{22}b_{22} + a_{23}b_{32} \end{bmatrix}\] <p>In general, multiplying a matrix <strong><em>X</em></strong> of shape $B \times D$ with <strong><em>W</em></strong> of shape $D \times K$, produces <strong><em>Y</em></strong> of shape $B \times K$: $y_{ij} = \sum_{k=0}^{D-1} x_{ik} w_{kj}$.</p> <p>Each element requires</p> <ul> <li>$D$ multiplications.</li> <li>$D-1$ summation (or $D$ summations if we consider creating <strong><em>Y</em></strong> in advance filled with zeros).</li> </ul> <p>Considering that we have $B \times K$ elements, the actual number of flops: <strong><em>2xBxDxK</em></strong> FLOPs.</p> <p><strong>Common FLOPs estimates:</strong></p> <ul> <li>Elemenwise operation on $m \times n$ matrix: <strong><em>O(mn)</em></strong> FLOPs</li> <li>Addition of two $m \times n$ matrices: <strong><em>m x n</em></strong> FLOPs</li> <li>In generaal, <strong>matrix multiplication</strong> is by far the <strong>most expensive</strong> operation in Deep Learning (for large enough matrices). And it is for what GPUs are designed and optimized (large enough matrices).</li> </ul> <p><strong>Interpretation:</strong></p> <ul> <li><strong><em>B</em></strong> is the number of data pointes</li> <li><strong><em>(DK)</em></strong> is the number of parameters</li> <li>FLOPs <strong>for forward pass</strong> is <strong><em>~2 (# tokens) (# parameters)</em></strong> This generalizes for Transformers as well.</li> </ul> <h3 id="benchmarking-actual-flops">Benchmarking Actual FLOPs</h3> <p>To measure actual FLOP/s (wall-clock time), we can time the matrix multiplication and compare against theoretical FLOP/s.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def time_matmul(a: torch.Tensor, b: torch.Tensor) -&gt; float:
    """Returns the number of seconds required to perform 'a @ b'."""

    # Wait until previous CUDA threads are done
    if  torch.cuda.is_available():
        corc.cuda.synchronize()

    def run():
        # performs the operation
        a @ b

        # Wait until previous CUDA threads are done
        if  torch.cuda.is_available():
            torch.cuda.synchronize()

    # Time the operation times
    num_trials = 5
    total_time = timeit.timeit(run, number = num_trials)

    return total_time / num_trials

actual_time = time_matmul(x, w)
actual_flop_per_sec = actual_num_flops / actual_time
</code></pre></div></div> <p>Each GPU has official FLOPs specs that report the peak performance:</p> <ul> <li>A100 [<a href="https://www.nvidia.com/en-us/data-center/a100/">spec</a>]</li> <li>H100 <a href="https://www.nvidia.com/en-us/data-center/h100/">spec</a></li> <li>RTX5090 <a href="https://www.nvidia.com/en-us/geforce/graphics-cards/50-series/rtx-5090/">spec</a></li> </ul> <h3 id="model-flops-utilization-mfu">Model FLOPs utilization (<strong>MFU</strong>)</h3> <p><strong>Definition:</strong> It shows <em>how well you squeeze hardware</em>, it is calculated as \(\frac{\text{actual FLOP/s}}{\text{promised FLOP/s}}\) (ignore communication and overhead).<br/> <strong>Note:</strong> Usually</p> <ul> <li>if MFU $\geq 0.5$, it is considered to be very good utilization.</li> <li>if MFU $\leq 0.05$, it is considered to be poor utilization.</li> <li>MFU is higher, when <strong>matrix multiplications dominate</strong>.</li> </ul> <hr/> <h2 id="conclusion">Conclusion</h2> <ul> <li><strong>Tensors in PyTorch</strong> are pointers to memory — reshaping, slicing, and transposing share storage unless you explicitly copy.</li> <li>Always know <strong>where your tensors live</strong> (CPU/GPU) and whether they’re contiguous, to avoid performance inefficiencies.</li> <li><strong>Mat. mul. dominates</strong> compute: for two matrices of shapes $M \times N$ and $N \times P$, the FLOPs = $2 \times M \times N \times P$.</li> <li><strong>FLOP/s depend</strong> on hardware (H100 &gt; A100) and data type (bfloat16 &gt; float32).</li> <li><strong>Model FLOPs utilization (MFU)</strong> measures how efficiently your model uses hardware: \(\text{MFU} = \frac{\text{actual FLOPs}}{\text{theoretical peak FLOPs}}\).</li> </ul>]]></content><author><name></name></author><category term="cs336"/><category term="dl-basics,"/><category term="lecture-2"/><summary type="html"><![CDATA[A deep dive into tensors, their storage, views, and compute in PyTorch.]]></summary></entry><entry><title type="html">How Computers Store Data in Memory: Brief Intro</title><link href="https://timashov.ai/blog/2025/data-in-memory/" rel="alternate" type="text/html" title="How Computers Store Data in Memory: Brief Intro"/><published>2025-04-26T09:00:00+00:00</published><updated>2025-04-26T09:00:00+00:00</updated><id>https://timashov.ai/blog/2025/data%20in%20memory</id><content type="html" xml:base="https://timashov.ai/blog/2025/data-in-memory/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>When <strong>we communicate</strong> with each other, we use complex <strong>natural languages</strong> like English, Portuguese, or Russian. The language we use depends on our location or the community around us. When we see something, we perceive information visually through our eyes and brain. And when we count numbers, most people are familiar with one system — the <strong>decimal system</strong>, where each position can be represented by one of 10 digits: <code class="language-plaintext highlighter-rouge">0, 1, 2, ..., 9</code>. So, at least three independent ways exist to perceive and exchange information.<br/> <strong>Computers</strong>, however, operate very differently — and in some sense, much more simply. They do not have a native natural language or vision system at their core. Instead, they use only the <strong>binary system</strong>, composed of <code class="language-plaintext highlighter-rouge">0</code> and <code class="language-plaintext highlighter-rouge">1</code>. Each “box” of information can contain either a 0 or a 1, and this unit is called a <strong>bit</strong>. Historically, 8 bits make up a <strong>byte</strong>.</p> <p><img src="/assets/img/bits_bytes.png" alt="Img.1: Explanation of Bits and Bytes" style="width:100%;"/></p> <p>Higher-level constructs, such as <strong>Natural Language Models</strong> (<em>LLaMA, ChatGPT, Grok, etc.</em>) and <strong>Vision Systems</strong> (<em>YOLO, Faster R-CNN, Stable Diffusion, etc.</em>), are built on top of this fundamental binary representation.</p> <p>Understanding how computers “speak” at the basic level is critical for building intuition in Deep Learning. In this post, I learn how computers represent and store information, how it connects to data types, and why these foundations matter when designing and optimizing AI models.</p> <hr/> <h2 id="decimal-vs-binary">Decimal vs Binary</h2> <p>The <strong>decimal system</strong> is the numeral system we use in daily life. Each digit can take one of <em>10</em> different values: <code class="language-plaintext highlighter-rouge">0, 1, 2, ..., 9</code>, ordered naturally from smallest to largest. To construct numbers, we use powers of <em>10</em>: when increasing past <em>9</em>, we reset the digit to <em>0</em> and add <em>1</em> to the next higher place value. The <strong>binary system</strong> works similarly — but instead of <em>10</em> possible values, each digit (bit) can only be <code class="language-plaintext highlighter-rouge">0</code> or <code class="language-plaintext highlighter-rouge">1</code>. Here, the base is <em>2</em>, and each digit represents a power of <em>2</em>.</p> <p><img src="/assets/img/nums_seq.png" alt="Img.2: Sequence of numbers in Decimal and Binary systems" style="width:100%;"/></p> <p>Fractional numbers follow the same idea: each digit after the floating point represents a negative power of the base - <em>10</em> for decimal numbers, and <em>2</em> for binary numbers. The algorithm for <strong>converting a fractional number from decimal to binary</strong> is iterative and slightly different for the integer and fractional parts:</p> <ul> <li>For the <strong>integer part</strong>, divide the number by <em>2</em>, record the remainder (<em>0</em> or <em>1</em>), and continue dividing the quotient by <em>2</em> <strong>until</strong> it becomes <em>0</em>.</li> <li>For the <strong>fractional part</strong>, multiply the fraction by <em>2</em>, record the integer part (<em>0</em> or <em>1</em>), and repeat the process with the remaining fractional part. The process <strong>stops when</strong> the fraction becomes 0 or when the desired precision is reached.</li> </ul> <p>Below is the step-by-step calculation for the number <code class="language-plaintext highlighter-rouge">13.875</code>:</p> <p><img src="/assets/img/binary_repr.png" alt="Img.3: Representation of fractional numbers in Decimal and Binary systems" style="width:100%;"/></p> <p>We’re used to the decimal system; however, the binary system follows the exact same logic — just with <em>2</em> symbols instead of <em>10</em>. Inside a computer, tiny transistors act as switches that can be either ON or OFF, determined by voltage levels. It’s much easier and more reliable to distinguish just two states — high vs. low voltage — than to detect multiple. This simplicity and robustness is <strong>why all information in computers is stored in binary</strong>.</p> <h2 id="main-data-types">Main Data Types</h2> <p>At the core of computing, we work with a few basic types:</p> <ul> <li><strong>Integer (<code class="language-plaintext highlighter-rouge">int</code>)</strong>: Whole numbers like <code class="language-plaintext highlighter-rouge">-17</code>, <code class="language-plaintext highlighter-rouge">0</code>, <code class="language-plaintext highlighter-rouge">256</code>, etc.</li> <li><strong>Floating-point (<code class="language-plaintext highlighter-rouge">float</code>)</strong>: Numbers with decimals, like <code class="language-plaintext highlighter-rouge">3.1415</code> or <code class="language-plaintext highlighter-rouge">-0.00127</code>.</li> <li><strong>Boolean (<code class="language-plaintext highlighter-rouge">bool</code>)</strong>: <code class="language-plaintext highlighter-rouge">True</code> or <code class="language-plaintext highlighter-rouge">False</code>, used in logical decisions.</li> <li><strong>Character (<code class="language-plaintext highlighter-rouge">char</code>)</strong>: Single characters like <code class="language-plaintext highlighter-rouge">'a'</code>, <code class="language-plaintext highlighter-rouge">'Z'</code>, <code class="language-plaintext highlighter-rouge">'Я'</code>, <code class="language-plaintext highlighter-rouge">'+'</code>.</li> <li><strong>String (<code class="language-plaintext highlighter-rouge">str</code>)</strong>: Sequences of characters like <code class="language-plaintext highlighter-rouge">"Capybara likes cuddling"</code>.</li> </ul> <p>Each type has a pre-allocated size that depends on the programming language and implementation details. For example:</p> <ul> <li><code class="language-plaintext highlighter-rouge">int8</code> (8-bit integer) has a size of <em>1 byte</em> and can represent values between <code class="language-plaintext highlighter-rouge">-128</code> (<code class="language-plaintext highlighter-rouge">-2^7</code>) and <code class="language-plaintext highlighter-rouge">127</code> (<code class="language-plaintext highlighter-rouge">2^7 - 1</code>).</li> <li>A regular <code class="language-plaintext highlighter-rouge">int</code> in Python has a size of <em>28 bytes</em> — which may seem surprisingly large. This is because Python integers include significant metadata (type, reference counts, etc.) and allocate memory dynamically.</li> </ul> <p>One can check the size of an object <code class="language-plaintext highlighter-rouge">x</code> in Python with <code class="language-plaintext highlighter-rouge">sys.getsizeof(x)</code>. However, it measures the full size, <strong>including Python’s internal overhead</strong>, which makes pure Python relatively inefficient for data storage and processing. To overcome this, optimized libraries like <strong>NumPy</strong> and <strong>PyTorch</strong> are used. For instance, <strong>NumPy</strong> provides fixed-size types like <code class="language-plaintext highlighter-rouge">int8</code>, <code class="language-plaintext highlighter-rouge">int16, int32</code>, etc. We can check the size of a NumPy object (e.g., <code class="language-plaintext highlighter-rouge">x = np.int16(2)</code>) using <code class="language-plaintext highlighter-rouge">x.nbytes</code>, which gives the actual payload size.</p> <hr/> <p><strong>Strings</strong> are represented using <em>bytes</em>, interpreted according to the <strong>character encoding</strong> — the system that maps bits to characters.</p> <p><strong>ASCII</strong> (American Standard Code for Information Interchange) uses <em>one byte per character</em> and defines 256 symbols (128 in the original standard). For example, the string <code class="language-plaintext highlighter-rouge">"Capybara"</code> is be encoded in ASCII as <code class="language-plaintext highlighter-rouge">[67, 97, 112, 121, 98, 97, 114, 97]</code>.</p> <p>When more characters are needed (such as emojis, Chinese ideograms, or mathematical symbols), modern systems use <strong>Unicode</strong>, which assigns a unique code point to every character. To actually store these code points as <em>bytes</em>, <strong>encodings</strong> like <strong>UTF-8</strong>, <strong>UTF-16</strong>, or <strong>UTF-32</strong> are used. In <strong>UTF-8</strong>, each character uses <strong>1 to 4 bytes</strong>, depending on the numerical value of the Unicode code point; and <em>UTF8</em> is backward compatible with ASCII. <em>Version 16.0</em> of the standard defines <em>154,998</em> characters across <em>168</em> scripts.</p> <p>Understanding Unicode and encoding schemes is critical in areas like <strong>Natural Language Processing (NLP)</strong>, where text from multiple languages must be handled efficiently and reliably.</p> <hr/> <h2 id="data-types-used-in-deep-learning">Data Types Used in Deep Learning</h2> <p><strong>Deep learning</strong> relies almost entirely on <strong>numerical tensors</strong>. Everything — models, inputs, outputs, intermediate activations — is stored as tensors, and most of them are <strong>floating-point based</strong>.</p> <p>The most common types:</p> <ul> <li><strong><code class="language-plaintext highlighter-rouge">float32</code></strong>: Single-precision (32-bit floating point) — the default choice for most models.<br/> Drawback: large memory usage (both GPU and RAM).</li> <li><strong><code class="language-plaintext highlighter-rouge">float16</code></strong>: Half-precision (16-bit floating point) — used for faster computation and reduced memory footprint.<br/> Challenge: lower dynamic range, can cause instability (overflow and underflow). Especially important for larger models. Used much less nowadays.</li> <li><strong><code class="language-plaintext highlighter-rouge">bfloat16</code></strong>: Brain Floating Point, designed specifically for deep learning. It addresses the issue with <code class="language-plaintext highlighter-rouge">float16</code> - it keeps the larger dynamic range (the same as <code class="language-plaintext highlighter-rouge">float32</code>), but with reduced fractional precision — better for training stability with less memory. It is <strong>good enough</strong> for forward pass computations.<br/> Challenge: for storing optimizer states and parameters, you still need to use <code class="language-plaintext highlighter-rouge">float32</code>.</li> <li><strong><code class="language-plaintext highlighter-rouge">fp8</code></strong>: 8-bit floating point - a very recent innovation, available on NVIDIA <code class="language-plaintext highlighter-rouge">H100</code> GPUs.<br/> Still experimental and not widely adopted.</li> <li><strong><code class="language-plaintext highlighter-rouge">int8</code></strong>: 8-bit integers — used in quantized models to reduce size and speed up inference.</li> </ul> <p><img src="/assets/img/fp_visual.png" alt="Img.4: Representation of data types in memory" style="width:100%;"/></p> <h4 id="floating-point-internals">Floating Point Internals</h4> <p>Floating point formats control the following two things:</p> <ul> <li><strong>Dynamic range</strong>: how far the binary point shifts — <strong>up to 127 positions</strong> in either direction. We subtract a <strong>bias of 127</strong> (binary 01111111) to allow both positive and negative shifts.</li> <li><strong>Fractional part</strong>: how finely numbers can be distinguished (handled by the mantissa bits).</li> </ul> <p>For example:</p> <ul> <li><code class="language-plaintext highlighter-rouge">fp8 E4M3</code> (4 exponent bits, 3 mantissa bits) can represent numbers like <code class="language-plaintext highlighter-rouge">11110</code> and <code class="language-plaintext highlighter-rouge">0.0001111</code> (3+1 meaningful bits).</li> <li><code class="language-plaintext highlighter-rouge">fp8 E5M2</code> (5 exponent bits, 2 mantissa bits) can represent <code class="language-plaintext highlighter-rouge">111000</code> and <code class="language-plaintext highlighter-rouge">0.0000111</code> (2+1 meaningful bits).</li> </ul> <p>The value of <code class="language-plaintext highlighter-rouge">float32</code> number is calculated as (IEEE 754):</p> \[N = (-1)^{b_{31}} \times 2^{(b_{30}b_{29} \dots b_{23})_2-127} \times (1.b_{22}b_{21} \dots b_0)_2\] <p>We can also write the exponent part in binary directly (more intuitive, if we compare with decimal logic):</p> \[N = (-1)^{b_{31}} \times 2^{(b_{30}b_{29} \dots b_{23})-01111111} \times (1.b_{22}b_{21} \dots b_0)\] <h4 id="quick-back-of-envelope-calculation">Quick Back-of-Envelope Calculation</h4> <p>Suppose we have a tensor <code class="language-plaintext highlighter-rouge">x</code> with shape <code class="language-plaintext highlighter-rouge">(batch_size=32, channels=3, height=224, width=224)</code>, typical for an image recognition model.</p> <p>How much memory would this tensor consume with different types?</p> <ul> <li> <p>Number of elements (<code class="language-plaintext highlighter-rouge">x.numel()</code> in <strong>PyTorch</strong>):<br/> <code class="language-plaintext highlighter-rouge">32 × 3 × 224 × 224 = 4,816,896</code></p> </li> <li> <p>Memory usage:</p> <ul> <li><code class="language-plaintext highlighter-rouge">float32</code> (4 bytes per value):<br/> <code class="language-plaintext highlighter-rouge">4,816,896 × 4 = ~ 18.4 MB</code></li> <li><code class="language-plaintext highlighter-rouge">float16</code> (2 bytes per value):<br/> <code class="language-plaintext highlighter-rouge">4,816,896 × 2 = ~ 9.2 MB</code></li> <li><code class="language-plaintext highlighter-rouge">int8</code> (1 byte per value):<br/> <code class="language-plaintext highlighter-rouge">4,816,896 × 1 = ~ 4.8 MB</code></li> </ul> </li> </ul> <p><strong>Notice</strong> Changing the data type immediately halves or quarters the memory footprint.</p> <hr/> <p>Choosing the right type has a huge impact on model <strong>speed</strong>, <strong>memory usage</strong>, and <strong>training stability</strong>.</p> <ul> <li>Training with <code class="language-plaintext highlighter-rouge">float32</code> is safe but memory-hungry.</li> <li>Switching to <code class="language-plaintext highlighter-rouge">float16</code>, <code class="language-plaintext highlighter-rouge">fp8</code>, and <code class="language-plaintext highlighter-rouge">bfloat16</code> saves memory and speeds up computation, but introduces <strong>training instability</strong>.</li> <li>Solution: to use <strong>mixed precision</strong> — selectively combining different types to balance memory usage and training stability.</li> </ul> <hr/> <h2 id="conclusion">Conclusion</h2> <p>Understanding data types is the foundation of building efficient AI systems.<br/> It affects not only how we design models but also how fast and how large they can be.</p> <p>In future posts, I’ll dive deeper into <strong>resource accounting</strong> — covering both <strong>memory</strong> and <strong>FLOPS</strong>. For memory, I’ll go beyond inputs to include gradients, intermediate activations, and other internal components of deep learning models.</p>]]></content><author><name></name></author><category term="cs336"/><category term="pre-dl,"/><category term="lecture-1"/><summary type="html"><![CDATA[fp32, fp16, and more]]></summary></entry><entry><title type="html">PyG Implementation of EDP-GNN: Generation via Score-Based Generative Modeling</title><link href="https://timashov.ai/blog/2024/pyg-implementation-of-edp-gnn-generation-via-score-based-generative-modeling/" rel="alternate" type="text/html" title="PyG Implementation of EDP-GNN: Generation via Score-Based Generative Modeling"/><published>2024-10-24T21:02:32+00:00</published><updated>2024-10-24T21:02:32+00:00</updated><id>https://timashov.ai/blog/2024/pyg-implementation-of-edp-gnn-generation-via-score-based-generative-modeling</id><content type="html" xml:base="https://timashov.ai/blog/2024/pyg-implementation-of-edp-gnn-generation-via-score-based-generative-modeling/"><![CDATA[<p>By Yiwen Chen, Aleksandr Timashov, and Yue (Andy) Zhang as part of the Stanford CS224W course project.</p> <h3>Table of Contents</h3> <ol><li>Motivation</li><li>Graph Neural Networks</li><li>Score-Based Generative Modeling</li><li>Annealed Langevin dynamic sampling</li><li>Noise Scheduler Utility</li><li>Edgewise Dense Prediction Graph Neural Network (EDP-GNN)</li><li>Conclusion</li><li>References</li></ol> <h3>Motivation</h3> <p>Recent advancements in generative models, particularly score-based generative modeling, have made significant strides across various domains. Yet, a key challenge in graph generation models remains: the lack of permutation invariance. This blog post introduces the EDP-GNN architecture, a permutation invariant approach to graph generation. EDP-GNN, once trained on a graph dataset, enables the generation of new graphs that mirror the input distribution using annealed Langevin dynamics sampling.<br/>In the context of score-based generative modeling, we focus on learning the gradient of log data density (score function). This approach permits more expressive architectures as it bypasses the need for score normalization. By leveraging inherently permutation-invariant structures like graph neural networks, we ensure consistent outputs for equivalent adjacency matrices. The EDP-GNN architecture exemplifies this, learning graph distribution scores while maintaining permutation invariance. Our discussion will also cover the practical aspects of training and sampling in score-based graph generation.</p> <h3>Graph Neural Networks</h3> <h4>Use cases</h4> <p>Graph Neural Networks (GNNs) are a major innovation in machine learning, tailored to analyze data structured like networks or graphs. This makes them ideal for a wide range of applications. This includes things like social networks and the way proteins fold, which are naturally structured as interconnected points. GNNs are crucial in areas like traffic management, where they help optimize routes by understanding complex road networks. They also play a significant role in finance, for fraud detection by analyzing transaction networks, and in recommendation systems, where they suggest products based on a user’s network of interests. What makes GNNs stand out is their ability to interpret the intricate web of relationships in these data sets, a task that conventional neural networks find challenging. By harnessing the power of connections and patterns within the data, GNNs offer insightful and effective solutions across these diverse fields, demonstrating their versatility and importance in handling graph-based information.</p> <h4>Permutation invariance</h4> <p>One of the key distinctions of GNNs from other neural network architectures is their ability to maintain permutation invariance. This means that the output of a GNN does not change even if the order of the nodes in the input graph is altered. This property is crucial in graph data since the ordering of nodes in a graph is often arbitrary and does not convey meaningful information. In a graph with N nodes, there can be up to N! (N factorial) different adjacency matrices representing the same graph. The invariance to node permutation allows GNNs to generalize better and be applied to various graph structures without losing effectiveness. The versatility and adaptability of GNNs in handling diverse and complex graph-structured data underscore their growing importance in the field of artificial intelligence.</p> <h3>Score-Based Generative Modeling</h3> <p>Score-based models for graph analysis use neural networks to model a unique aspect called the score function. This function essentially represents the gradient of the logarithm of a graph’s data distribution. The advantage of using the score function lies in its simplicity: unlike direct density modeling, which requires complex normalization to ensure probabilities sum to one, score functions bypass this requirement.</p> <p>These models are structured around a sequence of noise levels, denoted as <em>σ</em>₁, <em>σ</em>₂​, …, <em>σ</em>ₗ, with each level being progressively smaller (<em>σ</em>₁ &lt; <em>σ</em>₂​ &lt; … &lt; <em>σ</em>ₗ). The model is conditioned on these noise levels, using them to gradually shape the output. Specifically, the model generates adjacency matrices by starting from noise and applying a technique known as annealed Langevin dynamics. This process, which we’ll explore in more detail, is key to understanding how score-based models efficiently and effectively handle graph data.</p> <h3>Langevin dynamic sampling</h3> <p>Once an EDP-GNN model is trained on a dataset of graphs, we can use annealed Langevin dynamics to sample new graphs from the model.<br/>For a given EDP-GNN score model <strong><em>s(x, σ)</em></strong>, the Langevin update used for sampling is:</p> <ol><li>Sample noise <strong><em>zₜ</em></strong> from Gaussian <strong><em>N(0, I)</em></strong></li><li>Update <strong><em>xₜ₊₁ = xₜ + α/2 s(x, σ) + √(α) zₜ</em></strong></li></ol> <p>We see that there are two key components in the update procedure. The term <strong><em>α/2 s(x, σ)</em></strong> is the gradient update, moving <strong><em>xₜ</em></strong> towards more likely regions of the learned data distribution. The term <strong><em>√(α) zₜ</em></strong> adds some noise to the sampling — without it, samples will converge to the local optima and you would not get correct samples. This procedure is run for a large number of iterations, <strong>T</strong>, while the step size alpha should be sufficiently small.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/864/1*tJuk1mTpawoBuVcPptDOAA.gif"/><figcaption>Annealed Langevin dynamics © Yang Song</figcaption></figure> <p>To extend the Langevin sampling procedure to annealed Langevin sampling, we make use of the noise conditioning of our EDP-GNN score model. We start by using the plain Langevin procedure to sample from a very noisy distribution <strong><em>s(x, σ_L)</em></strong>. The samples produced are then used to initialize the sampling from <strong><em>s(x, σ_{L-1})</em></strong>, which provides slightly less noisy gradient updates. We gradually decrease the noise and refine the samples until <strong><em>s(x, σ</em></strong>₁<strong><em>)</em></strong>, which has the most accurate representation of the input data distribution.<br/>The full pseudocode for annealed Langevin dynamics sampling as follows:</p> <pre>Initialize x_1 from a simple prior distribution, eg. Gaussian<br />For sigma_i in sigma_L, …, sigma_1:<br /> Define the step size alpha_i  = epsilon * sigma_i^2 / sigma_L^2<br /> For t in 1, … T:<br />  Sample z_t from N(0, I)<br />  Update x_{t+1} = x_t + alpha/2 * s(x, sigma) + sqrt(alpha) z_t<br /> Initialize for the next noise level x_1 = x_t<br />Return x_T</pre> <p>In pytorch_geometric, annealed Langevin dynamics sampling for EDP-GNN (and other score-based graph generative models) is implemented in <em>torch_geometric.utils.langevin.</em> You can use it like this:</p> <pre>import torch_geometric.utils.langevin as langevin<br />score_model = … # EDP-GNN model<br />node_features = … # num_nodes x feature_dim matrix of node features<br />adjs, node_flags = langevin.generate_initial_sample(<br />  batch_size=5, num_nodes=10<br />)<br /><br />def score_func(adjs, node_flags):<br /> return score_model(node_features, adjs, node_flags)<br /><br />new_adjs = langevin.sample(<br />  score_func, adjs, node_flags, num_steps=5000, quantize=True<br />)</pre> <p>In the pytorch_geometric implementation, a couple of graph-specific modifications are made to the annealed Langevin dynamics procedure:</p> <ol><li>The adjacency matrices produced by this procedure have continuous values, but in practice, we often wish to sample graphs with discrete edge values. We can do this at the end, by simply defining a threshold to binarize or quantize the adjacency matrix values.</li><li>Since adjacency matrices are symmetric, the initial adjacency matrix x_1 and noise term z_t also must be made symmetric.</li></ol> <h3>Noise Scheduler Utility</h3> <p>The noise scheduler is a commonly used utility for generating noise levels in score models and other diffusion-based models. In score-based generative modeling framework, noise is systematically added to the data at various levels, denoted as sigma values, during the training process. This approach is essential for learning the noise-conditional score model. We have implemented the noise scheduler as described in “Generative Modeling by Estimating Gradients of the Data Distribution”, which specifically generates noise levels on a logarithmic scale.</p> <pre>def get_smld_sigma_schedule(<br />    sigma_min: float,<br />    sigma_max: float,<br />    num_scales: int,<br />    dtype: Optional[torch.dtype] = None,<br />    device: Optional[torch.device] = None,<br />) -&gt; Tensor:<br />    r&quot;&quot;&quot;Generates a set of noise values on a logarithmic scale for &quot;Score<br />    Matching with Langevin Dynamics&quot; from the `&quot;Generative Modeling by<br />    Estimating Gradients of the Data Distribution&quot;<br />    &lt;https://arxiv.org/abs/1907.05600&gt;`_ paper.<br /><br />    This function returns a vector of sigma values that define the schedule of<br />    noise levels used during Score Matching with Langevin Dynamics.<br />    The sigma values are determined on a logarithmic scale from<br />    :obj:`sigma_max` to :obj:`sigma_min`, inclusive.<br /><br />    Args:<br />        sigma_min (float): The minimum value of sigma, corresponding to the<br />            lowest noise level.<br />        sigma_max (float): The maximum value of sigma, corresponding to the<br />            highest noise level.<br />        num_scales (int): The number of sigma values to generate, defining the<br />            granularity of the noise schedule.<br />        dtype (torch.dtype, optional): The output data type.<br />            (default: :obj:`None`)<br />        device (torch.device, optional): The output device.<br />            (default: :obj:`None`)<br />    &quot;&quot;&quot;<br />    return torch.linspace(<br />        math.log(sigma_max),<br />        math.log(sigma_min),<br />        num_scales,<br />        dtype=dtype,<br />        device=device,<br />    ).exp()</pre> <p>To utilize this function, it is necessary to specify the range and number of sigma values required for the schedule. The determination of these parameters depends on the specific needs of the model and the data. For example, small-scale graph generation with EDP-GNN in Niu 2020 uses six noise levels, while image generation models may require hundreds or thousands of noise levels. It often involves a process of experimentation and tuning to identify the most effective schedule for a given dataset and model architecture.</p> <p>The noise scheduler used in diffusion models, as detailed in the paper “Denoising Diffusion Probabilistic Models,” shares conceptual similarities with the one in Score Matching with Langevin Dynamics. However, it serves a different purpose and operates under different mechanics. In diffusion models, the scheduler generates a sequence of beta values for use in the forward diffusion process. Although this particular type of noise scheduler was not directly utilized in our project, we have implemented some variants to facilitate future diffusion-based graph models.</p> <h3>Edgewise Dense Prediction Graph Neural Network (EDP-GNN)</h3> <h4><strong>Multi-Channel GNN Layer</strong></h4> <p>The Multi-channel GNN layer, an extension of the Graph Isomorphism Network (GIN) layer, employs a novel approach to message passing. Its core idea involves simultaneously running message-passing algorithms over identical node features, but with different adjacency matrices. This process culminates in an output that is a concatenation of the outputs from all C channels, the number of channels is equal to the number of intermediate adjacency matrices. The formula for the m-th layer in this setup, tailored for a C-channel adjacency matrix input is below:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/437/1*jBSxC-S_hbpLPpPe1rVyeA.png"/></figure> <p>Notably, the Multi-channel GNN layer is a fundamental component of the final EDP-GNN model, enabling it to handle complex graph structures efficiently.<br/>The main parts of the implementations are message and forward functions.</p> <pre>    def message(self, x_j, edge_weight):<br />        return x_j * edge_weight<br /><br />    def forward(self, x: Union[Tensor, OptPairTensor], edge_indices: List[Adj],<br />                edge_weights: List[Tensor], size: Size = None) -&gt; Tensor:<br /><br />        ...<br />        edge_weights_cat = torch.cat(edge_weights, dim=-1)[:, None]<br />        # duplicate features to run over C channels<br />        <br />        ...<br />        # propagate_type: (x: OptPairTensor)<br />        out_cat = self.propagate(edge_index_cat, edge_weight=edge_weights_cat,<br />                                 x=x_cat, size=size)<br /><br />        x_r = x_cat[1]<br />        if x_r is not None:<br />            out_cat = out_cat + (1 + self.eps) * x_r  # N * C x F_in<br /><br />        # reshape to get concatenated features for each of C channels<br />        out_cat = out_cat.reshape((C, N, -1)).permute((1, 0, 2))<br />        out = out_cat.reshape((N, -1))<br /><br />        return self.nn(out)</pre> <h4>EDP-GNN layer</h4> <p>The EDP-GNN layer functions by processing an input consisting of a C-channel adjacency matrix accompanied by node features and outputs an updated C’-channel adjacency matrix also with node features.<br/>The computation of node features is conducted using the Multi-Channel GNN layer. Subsequently, a new C’-channel adjacency matrix is generated. This process involves utilizing the newly computed node features in conjunction with the original C-channel adjacency matrix. The method integrates these elements using concatenation and a Multilayer Perceptron (MLP) network. This approach underlines the layer’s efficiency in transforming and updating the graph structure. <br/>In this approach to modeling undirected graphs, it’s essential to maintain the symmetry of the predicted adjacency matrix. To achieve this, we sum the predicted matrix with its transposed version. This process ensures that the final adjacency matrix accurately reflects the undirected nature of the graph. For a more comprehensive understanding of this transformation, detailed formulas are provided below.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/280/1*BwL-eq_pz3lqnG3NYc6YBQ.png"/></figure> <h4>Final network architecture</h4> <p>The final network is represented by three layers as we can see on the image. Before being inputted into our EDP-GNN model, graphs undergo preprocessing. This involves using two-channel adjacency matrices: one channel for the original adjacency matrix and the other for its negated counterpart, with all entries inverted. Additionally, we initialize node features based on the weighted degrees of the nodes. This process ensures the graphs are optimally formatted for processing by the EDP-GNN model.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*KxzowzIrgtwOHsL5BSmV9A.jpeg"/><figcaption>High level of EDP-GNN with 3 layers</figcaption></figure> <p>Specifically, if we have node features <em>X</em> from the data, then the initial value for each node is as follows.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/199/1*iP3yLNhrdKckmhnav_TtnQ.png"/></figure> <p>A final point to note is that the dimensions of the inputs and outputs in our network are identical. This aspect is crucial because the network is designed to model the score function. Maintaining consistent dimensions ensures that the network accurately represents and processes the score function, which is vital for the model’s effectiveness.</p> <h4>Noise level conditioning</h4> <p>As previously mentioned, it’s necessary to condition each layer of our network on various noise levels. We achieve this noise conditioning by introducing a few learnable parameters, specifically by adding gains and biases. A conditional Multilayer Perceptron (MLP) layer is represented as:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/253/1*EX-fq9BgF1WcUL7FmMlLlQ.png"/></figure> <p>Where α and β are learnable parameters for each noise level σ.</p> <h3>Conclusion</h3> <p>In this post, we have explored the training and sampling process of a score-based graph generation method. This method leverages Graph Neural Networks (GNNs) to preserve the critical inductive bias of permutation invariance in the generated graphs. Furthermore, we demonstrated how the EDP-GNN network can be implemented using PyG utilities. If you want to delve deeper into the technical details, we encourage you to read the original paper available at <a href="https://arxiv.org/abs/2003.00638">https://arxiv.org/abs/2003.00638</a>. Thank you for joining us on this learning journey!</p> <h3>References</h3> <p>[1] C. Niu, Y. Song, J. Song, S. Zhao, A. Grover, and S. Ermon,<br/><a href="https://arxiv.org/abs/2003.00638">Permutation Invariant Graph Generation via Score-Based Generative Modeling</a>, (2020), AISTATS 2020<br/>[2] Song, Y. and Ermon, S. (2019). <a href="https://arxiv.org/abs/1907.05600">Generative modeling by estimating gradients of the data distribution</a>. arXiv preprint arXiv:1907.05600<br/>[3] Xu, K., Hu, W., Leskovec, J., and Jegelka, S. (2018a). <a href="https://arxiv.org/abs/1810.00826">How powerful are graph neural networks?</a> arXiv preprint arXiv:1810.00826<br/>[4] Jonathan Ho, Ajay Jain, Pieter Abbeel. Diffusion Probabilistic Models <a href="https://arxiv.org/abs/2006.11239">https://arxiv.org/abs/2006.11239</a><br/>[5] Yang Song. Diffusion and Score-Based Generative Models YouTube video. <a href="https://youtu.be/wMmqCMwuM2Q?si=64WID5_82zywM6_C">https://youtu.be/wMmqCMwuM2Q?si=64WID5_82zywM6_C</a></p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=e45c24d1ce89" width="1" height="1" alt=""/>&lt;hr&gt;&lt;p&gt;<a href="https://medium.com/stanford-cs224w/pyg-implementation-of-edp-gnn-generation-via-score-based-generative-modeling-e45c24d1ce89">PyG Implementation of EDP-GNN: Generation via Score-Based Generative Modeling</a> was originally published in <a href="https://medium.com/stanford-cs224w">Stanford CS224W: Machine Learning with Graphs</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</p>]]></content><author><name></name></author></entry><entry><title type="html">Live CNN Training Dashboard: Hyperparameters Tuning</title><link href="https://timashov.ai/blog/2020/live-cnn-training-dashboard-hyperparameters-tuning/" rel="alternate" type="text/html" title="Live CNN Training Dashboard: Hyperparameters Tuning"/><published>2020-11-14T14:17:06+00:00</published><updated>2020-11-14T14:17:06+00:00</updated><id>https://timashov.ai/blog/2020/live-cnn-training-dashboard-hyperparameters-tuning</id><content type="html" xml:base="https://timashov.ai/blog/2020/live-cnn-training-dashboard-hyperparameters-tuning/"><![CDATA[<h4><a href="https://towardsdatascience.com/tagged/hands-on-tutorials">Hands-on Tutorials</a></h4> <h3>Table of contents</h3> <ol><li><strong>Why we need to build a Live CNN Training Dashboard?</strong></li><li><strong>Introduction</strong></li><li><strong>Prerequisites</strong></li><li><strong>System description</strong></li><li><strong>How to create an environment and start training?</strong></li><li><strong>Conclusion</strong></li><li><strong>References</strong></li></ol> <h3>Why we need to build a Live CNN Training Dashboard?</h3> <p>When I studied in mathematical lyceum, my teacher taught me that the best way to understand something is to visualize it. For example, we had a wooden board, plasticine, and metal wire to be able to visualize stereometry problems. It helped a lot to develop visual thinking and skills in solving challenging tasks.</p> <p>I truly believe that real data scientists should understand algorithms and have a feeling on how to improve it if something works not fine. Especially in the area of deep learning. In my mind, the best way to develop these skills is to see how the model is trained, what happens when you change hyperparameters. This is the reason why I want to share how to build a simple dashboard for CNN live training with the opportunity to tune a few hyperparameters online.</p> <p>There is common knowledge that if we choose too big learning rate, we will see how our loss function explodes (our model will not converge); if we choose too small learning rate, the training process can last too long. What about dropout? There is an opinion that dropout reduces overfitting. I get to check everything myself even if I believe, because to know and to believe are different things.</p> <p>Below is the short demo of my dashboard. Red dots on loss function &amp; accuracy plots represent the training dataset, blue dots represent the test dataset.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/866/1*HhRidOhxiPrvrThYqqz3Pg.gif"/><figcaption>Image by the author</figcaption></figure> <h3>Introduction</h3> <p>Dashboard displays the following statistics:</p> <ul><li>loss function value in time;</li><li>accuracy in time;</li><li>distribution of activation maps values for the last step;</li><li>history of hyperparameters changes (table);</li></ul> <p>For this task, I am using AlexNet architecture to classify images on 10 classes: Alaskan malamute, baboon, echidna, giant panda, hippo, king penguin, llama, otter, red panda, and wombat. Images are downloaded from the ImageNet. I will not go into details in this post, but you can explore file <strong>get_dataset.py</strong>. During training, the following parameters can be tweaked:</p> <ul><li>optimizer;<br/>This parameter determines the algorithm we use to optimize our model. I use only Adam and SGD with Nesterov momentum. If you want to understand the optimization technique more, I encourage you to watch a video from Stanford <a href="https://www.youtube.com/watch?v=_JB0AO7QxSA">here</a>. There are many fantastic details about optimization.</li><li>learning rate;<br/>This parameter determines how fast we are moving down the slope when we are updating weights. For basic gradient descent formula for weights updates look like this: w := w — lr * dw.</li><li>weight decay;<br/>For our case it is simply L2 regularization: R(W) = SUM(W * W). It is considered that weight decay does not make a lot of sense in the context of CNN, but you can see it yourself how it works live. You can read some description of L1 and L2 regularization techniques <a href="https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c">here</a>.</li><li>dropout;<br/>Common regularization strategy for neural network. The idea is randomly set some neurons to zero on each training step. The hyperparameter is the probability to drop each neuron. Common value is 0.5 (50%). We can choose any integer value from 20 to 80. (in %) More details can be watched in the same video that I shared for optimizer.</li></ul> <p>Script can be easily changed to add additional functionality.</p> <h3>Prerequisites</h3> <p>I assume that you understand what is CNN and have basic knowledge of the following:</p> <ul><li>PostgreSQL (to store real-time data);</li><li>Dash (to build dashboard, <a href="https://plotly.com/dash/">https://plotly.com/dash/</a>);</li><li>PyTorch (to build CNN models);</li></ul> <h3>System description</h3> <p>There are four main parts of the system: <em>dataset, model, database, and dashboard/UI</em>. These parts interact with each other to successfully run the system. Firstly I will describe each of these parts and after that, I will give a short description of how they interact with each other.</p> <h4>Dataset</h4> <p>For this exercise, I use a dataset from the ImageNet that contains the following ten classes: <em>Alaskan malamute, baboon, echidna, giant panda, hippo, king penguin, llama, otter, red panda, and wombat. </em>To download all images from ImageNet, I can run python board.py from the following location: ../cnn_live_training.</p> <p>Firstly, I have to find classes ids and save them to some variable:</p> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/9f15dc13a20215b8bb39c30e5a55a92b/href">https://medium.com/media/9f15dc13a20215b8bb39c30e5a55a92b/href</a></iframe> <p>The ImageNet stores URLs to images. Some URLs/images might not exist anymore. To get these URLs based on class id, I use the following function:</p> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/e2e0b5d5a9bc2bf913270f868e053ede/href">https://medium.com/media/e2e0b5d5a9bc2bf913270f868e053ede/href</a></iframe> <p>To download all images I use a loop where I download image by image. Below is the function to download image by URL:</p> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/5e40c6ec93877779ada08f4bab23aae6/href">https://medium.com/media/5e40c6ec93877779ada08f4bab23aae6/href</a></iframe> <p>The full version of the code can be seen in the file <strong>get_dataset.py</strong>. You can easily change these classes to other classes or you can even change the ImageNet to your custom dataset.</p> <h4>Model</h4> <p>For the training, I am using by default the AlexNet architecture with Adam or SGD with Nesterov momentum optimizer. Optionally, the VGG16 can be chosen. Models can be imported either from the file <strong>models.py</strong> or from torchvision.models. The second option has the opportunity to use pre-trained weights. Dataset preparation happens in the file <strong>data_preparation.py</strong>. The training process happens in the file <strong>train.py</strong>.</p> <p>I don’t have the goal to explain in this article how to build a pipeline for training CNN that is why I am not going into detail in this part. But I am happy to recommend the amazing course CS231n from Stanford and particularly HW2(Q4), where you can learn step by step how to build this pipeline. This homework can be found <a href="https://cs231n.github.io/assignments2020/assignment2/">here</a>.</p> <h4>Database</h4> <p>Before running the system, we have to create <em>dl_playground</em> DB in PostgreSQL with the schema <em>cnn_live_training</em> that contains three following tables: <em>parameters, statistics, activations</em>.</p> <p><strong>parameters</strong><br/>This table contains <em>only one row</em> with current parameters for the training CNN model. When we change any parameters in our dashboard (file <strong>board.py</strong>), this data will be updated in the <em>parameters</em> SQL table. The table contains the following columns:</p> <ul><li>optimizer;<br/>Text data type. Can have two values: ‘Adam’ and ‘SGD+Nesterov’.</li><li>learning_rate;<br/>Double data type. The values are between 0 and 1 with the 0.00005 step.</li><li>weight_decay;<br/>Double data type. The values are between 0 and 1 with the 0.05 step.</li><li>dropout;<br/>Integer data type. The values are between 20 and 80. (It is assumed that the values are in %.)</li><li>dt_updates;<br/>Timestamp data type. Indicates date and time when data was modified.</li><li>stop_train;<br/>Boolean data type. Indicates if we have to stop training.</li></ul> <p><strong>statistics<br/></strong> This table contains statistics of the training process. Data is updated every --n-print step. The table contains the following columns:</p> <ul><li>dt_started;<br/>Timestamp data type. Indicates when current training was started.</li><li>model_name;<br/>Text data type. In this case, it can be only ‘MyAlexNet’.</li><li>epoch;<br/>Integer data type. Indicates the number of training epochs.</li><li>step;<br/>Integer data type. Indicates the number of training steps.</li><li>optimizer;<br/>Text data type. Can have two values: ‘Adam’ and ‘SGD+Nesterov’.</li><li>learning_rate;<br/>Double data type. The values are between 0 and 1 with the 0.00005 step.</li><li>weight_decay;<br/>Double data type. The values are between 0 and 1 with the 0.05 step.</li><li>dropout;<br/>Integer data type. The values are between 20 and 80.</li><li>dt;<br/>Timestamp data type. Indicates date and time when data was modified.</li><li>train_loss;<br/>Double data type. The value of loss function for the training dataset on the last step.</li><li>train_accuracy;<br/>Double data type. The value of accuracy for the training dataset on the last step.</li><li>validate_loss;<br/>Double data type. The value of loss function for the validation dataset on the last step.</li><li>validate_accuracy;<br/>Double data type. The value of accuracy for the validation dataset on the last step.</li></ul> <p><strong>activations</strong><br/>This table contains the current distribution of weights in activation maps for all convolutional and fully connected layers. The table contains the following columns:</p> <ul><li>nn_part;<br/>Text data type. Can be either ‘features’ or ‘classifier’.</li><li>layer_type;<br/>Text data type. Can be either ‘conv’ or ‘fc’.</li><li>number;<br/>Integer data type. Indicates the layer number in a ‘nn’ part.</li><li>weights;<br/>Double[] data type. Indicates average values of weights in bins.</li><li>num_weights;<br/>Integer[] data type. Indicates numbers of values in bins.</li></ul> <h4>Dashboard/UI</h4> <p>The dashboard consists of three main blocks: <em>control panel, loss function &amp; accuracy, and activation maps (distribution)</em>. These blocks are built using dash containers.</p> <p><strong>Control panel</strong> contains filters of parameters and “submit parameters” button that can be used to send chosen parameters to described above table “parameters”.<br/>There are four filters: optimizer, learning rate, weight decay, and dropout.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*gO3qCjmZ2W_t53D2BY5U9g.png"/><figcaption>Image by the author</figcaption></figure> <p>Below is the script, how to create an optimizer filter (other filters are similar):</p> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/4f8d477162beffe5461cdbbcc80f0906/href">https://medium.com/media/4f8d477162beffe5461cdbbcc80f0906/href</a></iframe> <p>After that I create a container that contains all four filters:</p> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/664f7649c8dde620e428b38602be1f62/href">https://medium.com/media/664f7649c8dde620e428b38602be1f62/href</a></iframe> <p>How to create other parts of the control panel can be found in the file <strong>board.py</strong>.</p> <p><strong>Loss function &amp; Accuracy</strong> contains a table with the history of used parameters and two plots with train/test loss function and accuracy values in time. Data is updated every one second (time interval can be changed) automatically.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*pnOp9JjL1Si19I9VCmsd-Q.png"/><figcaption>Image by the author</figcaption></figure> <p>Below is the script on how to create a table and button to stop training in the dashboard (I replaced real styles with short names for reading convenience):</p> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/3789925ae6f251b225c7c20a2732e11f/href">https://medium.com/media/3789925ae6f251b225c7c20a2732e11f/href</a></iframe> <p>Script to create plot template can be seen below:</p> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/7297eb93fdf91a4436782acec545f1d1/href">https://medium.com/media/7297eb93fdf91a4436782acec545f1d1/href</a></iframe> <p>Values are uploaded dynamically from PostgreSQL using callbacks (I provide only template for reading convenience):</p> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/96ca5ddf33d265257bd1131dc18c5246/href">https://medium.com/media/96ca5ddf33d265257bd1131dc18c5246/href</a></iframe> <p>I need to use a callback here because I want to update the plot and the table every 1 second. So, I have to use this variable as an input.</p> <p><strong>Activation maps (distribution)</strong> contains plots with distribution of activation map for each layer for the last step. Data is updated every one second (time interval can be changed) automatically.</p> <p>The activations of the first two layers look similar to a normal distribution with the mean value in 0. The reason for this is for the first two layers we apply normalization. To understand more, I encourage you to watch a lecture from Stanford <a href="https://www.youtube.com/watch?v=wEoyxE0GP2M">here</a>.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Hp19LuAebQOoa7F8wZL9dQ.png"/><figcaption>Image by the author</figcaption></figure> <p>Below is the script to create a container with the plots. It is similar to the previous container with loss function and accuracy plots:</p> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/6cc47a3ce1631f15e01eef05b1e11308/href">https://medium.com/media/6cc47a3ce1631f15e01eef05b1e11308/href</a></iframe> <p>The callback for the activation maps is similar to the “loss function &amp; accuracy”:</p> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/bd88a63b7ba7f0cc176093907b5265cd/href">https://medium.com/media/bd88a63b7ba7f0cc176093907b5265cd/href</a></iframe> <h4>How everything works</h4> <p>It’s time to wrap everything up. To recall back, my goal is to train CNN live and being able to control this process by changing hyperparameters. So how does it happen? I have a dashboard where we can see the progress of the CNN training and where we have some filters that we can choose and activate by pushing the button “Submit parameters”.</p> <p>What happens after that? All these parameters are sent to the table <em>parameters</em> in my database in PostgreSQL, using callback in the file <strong>board.py</strong> and function <em>update_params</em>:</p> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/3167abb99f196edc04396152c1b451bf/href">https://medium.com/media/3167abb99f196edc04396152c1b451bf/href</a></iframe> <p>At the same time, the script <strong>train.py</strong> connects to a database at the end of each training step, seeking to update the optimizer if parameters get updated:</p> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/7ed8b471a893e37137e9db49c348ab03/href">https://medium.com/media/7ed8b471a893e37137e9db49c348ab03/href</a></iframe> <p>Every <em>n_step</em> step data from training is saved to <em>statistics</em> and <em>activations</em> tables in database in PostgreSQL:</p> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/8a5190d919b9f9d5182aca2b93e1140a/href">https://medium.com/media/8a5190d919b9f9d5182aca2b93e1140a/href</a></iframe> <p>And this data simultaneously displayed in the dashboard because the script <strong>board.py</strong> every 1 sec. connects to the same tables:</p> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/d86dbc3de1f22ea60d0dd7fd5a9d4c88/href">https://medium.com/media/d86dbc3de1f22ea60d0dd7fd5a9d4c88/href</a></iframe> <p>All parameters are displayed in the table by extracting this information from the table :</p> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/d8ea4b68cdc1c63bf63ebfa0aeac795b/href">https://medium.com/media/d8ea4b68cdc1c63bf63ebfa0aeac795b/href</a></iframe> <p>If we want to stop training beforehand, we can push the button “Stop Training” below the table. After pushing the button, the callback will change the variable <em>stop_train</em> from <em>False</em> to <em>True</em> in the <em>parameters</em> table in my database:</p> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/cc7ed36586b8e1730acef94864e944a0/href">https://medium.com/media/cc7ed36586b8e1730acef94864e944a0/href</a></iframe> <p>At the same time, the script <strong>train.py</strong> check this parameter every training step and if it is <em>True</em>, training will be interrupted.</p> <p>Without practical recommendations on what parameters to use to start training, this post will not be complete. If you want to see that everything works, but don’t have time for experiments, you can start from the following parameters:</p> <ul><li>optimizer: Adam;</li><li>learning rate: 0.0003;</li><li>weight decay: 0;</li><li>dropout: 50%;</li></ul> <p>If you want to see how the model explodes, just increase the learning rate to 0.01. Good luck with your experiments.</p> <h3>How to create an environment and start training?</h3> <h4>Virtual environment setting up</h4> <p>I will give a short description for Ubuntu, using a virtual environment (<em>venv</em>).</p> <ol><li>Install Python 3.8: sudo apt install python3.8-minimal</li><li>Install virtual environment with Python 3.8: sudo apt-get install python3.8-venv</li><li>Create virtual environment: run from cnn_live_training folder: python3.8 -m venv venv</li><li>Activate environment: source venv/bin/activate</li><li>Install required packages in the virtual environment: <br/>pip install -r requirements.txt</li></ol> <h4>Collect dataset</h4> <p>Run from the ../cnn_live_training command python get_dataset.py</p> <h4>Start training</h4> <p>Run from the ../cnn_live_training folder two following commands</p> <pre>python board.py<br />python train.py</pre> <h3>Conclusion</h3> <p>In this story, I wanted to share my idea on how to nurture the feeling of training CNN. From one side, the idea is simple: build a training pipeline, create a dashboard and connect them using a database. But there are many annoying details that not possible to put in one small story. All script and additional details can be found in my <a href="https://github.com/atimashov/cnn_live_training">git repository</a>.</p> <p>If this post makes someone interested and give additional knowledge, I will become slightly happier because it means that I reached my goal. I will appreciate any comments, constructive criticism, or questions, feel free to leave your feedback below or you can reach me via <a href="https://www.linkedin.com/in/alexander-timashov/">LinkedIn</a>.</p> <h3>References</h3> <p>[1] L. Fei-Fei, R. Krishna and D. Xu, <a href="http://cs231n.stanford.edu/">CS231n: Convolutional Neural Networks for Visual Recognition</a> (2020), Stanford University</p> <p>[2] A. Krizhevsky, I. Sutskever and G. E. Hinton, <a href="https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf">ImageNet Classification with Deep Convolutional Neural Networks</a> (2012), NeurIPS 2012</p> <p>[3] A. Nagpal, <a href="https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c">L1 and L2 Regularization Methods</a> (2017), Towards Data Science</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=6b3382d9e44f" width="1" height="1" alt=""/>&lt;hr&gt;&lt;p&gt;<a href="https://medium.com/data-science/live-cnn-training-dashboard-hyperparameters-tuning-6b3382d9e44f">Live CNN Training Dashboard: Hyperparameters Tuning</a> was originally published in <a href="https://medium.com/data-science">TDS Archive</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</p>]]></content><author><name></name></author></entry></feed>